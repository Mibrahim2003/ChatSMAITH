{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from agents import Agent, WebSearchTool, Runner\n",
    "from agents.model_settings import ModelSettings\n",
    "\n",
    "# Initialize\n",
    "load_dotenv(override=True)\n",
    "client = OpenAI()\n",
    "\n",
    "# Create knowledge_files directory if it doesn't exist\n",
    "os.makedirs(\"knowledge_files\", exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc115c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SMART WEBSITE SCRAPER - PRIMARY SOURCE (Phase 3 Enhanced)\n",
    "# ============================================================\n",
    "\n",
    "# Keywords to identify important pages (expanded for various site types)\n",
    "IMPORTANT_PAGE_KEYWORDS = [\n",
    "    # Company/Business pages\n",
    "    'about', 'about-us', 'aboutus', 'who-we-are',\n",
    "    'services', 'service', 'what-we-do', 'solutions',\n",
    "    'products', 'product', 'offerings',\n",
    "    'contact', 'contact-us', 'contactus', 'get-in-touch',\n",
    "    'faq', 'faqs', 'help', 'support',\n",
    "    'team', 'our-team', 'leadership', 'people',\n",
    "    'pricing', 'plans', 'packages',\n",
    "    'features', 'benefits', 'why-us',\n",
    "    'blog', 'news', 'resources',\n",
    "    'careers', 'jobs', 'work-with-us',\n",
    "    # Personal/Academic websites\n",
    "    'publications', 'papers', 'research',\n",
    "    'projects', 'portfolio', 'work',\n",
    "    'resume', 'cv', 'bio', 'biography',\n",
    "    'talks', 'speaking', 'presentations',\n",
    "    'courses', 'teaching', 'education',\n",
    "    'books', 'articles', 'writing',\n",
    "    # Social/Connect pages\n",
    "    'connect', 'social', 'links',\n",
    "]\n",
    "\n",
    "MAX_PAGES_TO_SCRAPE = 10\n",
    "REQUEST_TIMEOUT = 15\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 1.0  # seconds between retries\n",
    "POLITE_DELAY = 0.5  # seconds between requests (rate limiting)\n",
    "USER_AGENT = \"ChatSMITH/1.0 (Website-to-Chatbot Generator; +https://github.com/chatsmith)\"\n",
    "\n",
    "# Cache for robots.txt to avoid re-fetching\n",
    "_robots_cache: Dict[str, set] = {}\n",
    "\n",
    "\n",
    "async def check_robots_txt(session: aiohttp.ClientSession, base_url: str) -> set:\n",
    "    \"\"\"\n",
    "    Fetch and parse robots.txt to get disallowed paths.\n",
    "    Returns a set of disallowed path prefixes for our user agent.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(base_url)\n",
    "    robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "    \n",
    "    # Check cache first\n",
    "    if parsed.netloc in _robots_cache:\n",
    "        return _robots_cache[parsed.netloc]\n",
    "    \n",
    "    disallowed = set()\n",
    "    try:\n",
    "        headers = {\"User-Agent\": USER_AGENT}\n",
    "        async with session.get(robots_url, headers=headers, timeout=aiohttp.ClientTimeout(total=5)) as response:\n",
    "            if response.status == 200:\n",
    "                text = await response.text()\n",
    "                \n",
    "                # Simple robots.txt parser - look for Disallow rules\n",
    "                current_agent = None\n",
    "                for line in text.split('\\n'):\n",
    "                    line = line.strip().lower()\n",
    "                    if line.startswith('user-agent:'):\n",
    "                        agent = line.split(':', 1)[1].strip()\n",
    "                        current_agent = agent\n",
    "                    elif line.startswith('disallow:') and current_agent in ['*', 'chatsmith']:\n",
    "                        path = line.split(':', 1)[1].strip()\n",
    "                        if path:\n",
    "                            disallowed.add(path)\n",
    "                \n",
    "                print(f\"  ü§ñ robots.txt: {len(disallowed)} disallowed paths\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Could not fetch robots.txt: {str(e)[:30]}\")\n",
    "    \n",
    "    # Cache the result\n",
    "    _robots_cache[parsed.netloc] = disallowed\n",
    "    return disallowed\n",
    "\n",
    "\n",
    "def is_path_allowed(url: str, disallowed_paths: set) -> bool:\n",
    "    \"\"\"Check if a URL path is allowed based on robots.txt rules.\"\"\"\n",
    "    if not disallowed_paths:\n",
    "        return True\n",
    "    \n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.lower()\n",
    "    \n",
    "    for disallowed in disallowed_paths:\n",
    "        if path.startswith(disallowed):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "async def fetch_page_with_retry(session: aiohttp.ClientSession, url: str, \n",
    "                                 retries: int = MAX_RETRIES) -> Tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Fetch a single page with retry logic.\n",
    "    Returns (url, html_content, error_message).\n",
    "    \"\"\"\n",
    "    last_error = \"\"\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            headers = {\"User-Agent\": USER_AGENT}\n",
    "            async with session.get(url, headers=headers, \n",
    "                                   timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT),\n",
    "                                   allow_redirects=True) as response:\n",
    "                \n",
    "                # Handle different status codes\n",
    "                if response.status == 200:\n",
    "                    html = await response.text()\n",
    "                    return url, html, \"\"\n",
    "                \n",
    "                elif response.status == 429:  # Rate limited\n",
    "                    wait_time = int(response.headers.get('Retry-After', 5))\n",
    "                    print(f\"  ‚è≥ Rate limited, waiting {wait_time}s...\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                    last_error = \"rate_limited\"\n",
    "                    continue\n",
    "                \n",
    "                elif response.status in [403, 401]:  # Forbidden/Unauthorized\n",
    "                    last_error = f\"access_denied_{response.status}\"\n",
    "                    break  # Don't retry auth errors\n",
    "                \n",
    "                elif response.status == 404:\n",
    "                    last_error = \"not_found\"\n",
    "                    break  # Don't retry 404s\n",
    "                \n",
    "                elif response.status >= 500:  # Server errors - retry\n",
    "                    last_error = f\"server_error_{response.status}\"\n",
    "                    if attempt < retries - 1:\n",
    "                        await asyncio.sleep(RETRY_DELAY * (attempt + 1))\n",
    "                        continue\n",
    "                \n",
    "                else:\n",
    "                    last_error = f\"http_{response.status}\"\n",
    "                    break\n",
    "                    \n",
    "        except asyncio.TimeoutError:\n",
    "            last_error = \"timeout\"\n",
    "            if attempt < retries - 1:\n",
    "                print(f\"  ‚è±Ô∏è Timeout for {url[:50]}..., retrying ({attempt + 1}/{retries})\")\n",
    "                await asyncio.sleep(RETRY_DELAY)\n",
    "                continue\n",
    "                \n",
    "        except aiohttp.ClientError as e:\n",
    "            last_error = f\"client_error: {str(e)[:30]}\"\n",
    "            if attempt < retries - 1:\n",
    "                await asyncio.sleep(RETRY_DELAY)\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            last_error = f\"error: {str(e)[:30]}\"\n",
    "            break\n",
    "    \n",
    "    if last_error:\n",
    "        print(f\"  ‚ùå Failed {url[:50]}...: {last_error}\")\n",
    "    return url, \"\", last_error\n",
    "\n",
    "\n",
    "# Keep the old function name for compatibility\n",
    "async def fetch_page(session: aiohttp.ClientSession, url: str) -> Tuple[str, str]:\n",
    "    \"\"\"Fetch a single page and return (url, html_content) - wrapper for compatibility\"\"\"\n",
    "    url, html, _ = await fetch_page_with_retry(session, url)\n",
    "    return url, html\n",
    "\n",
    "\n",
    "def clean_html_content(html: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Clean HTML and extract meaningful content.\n",
    "    Returns structured data with title, description, sections, and clean text.\n",
    "    \"\"\"\n",
    "    if not html:\n",
    "        return {\"title\": \"\", \"description\": \"\", \"sections\": [], \"content\": \"\"}\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'footer', 'header', \n",
    "                                   'aside', 'noscript', 'iframe', 'svg', 'form']):\n",
    "        element.decompose()\n",
    "    \n",
    "    # Remove elements by common class/id patterns (ads, popups, etc.)\n",
    "    noise_patterns = ['cookie', 'popup', 'modal', 'advertisement', 'ad-', 'sidebar', \n",
    "                      'newsletter', 'subscribe', 'social', 'share', 'comment']\n",
    "    for pattern in noise_patterns:\n",
    "        for element in soup.find_all(class_=lambda x: x and pattern in str(x).lower()):\n",
    "            element.decompose()\n",
    "        for element in soup.find_all(id=lambda x: x and pattern in str(x).lower()):\n",
    "            element.decompose()\n",
    "    \n",
    "    # Extract title\n",
    "    title = \"\"\n",
    "    if soup.title:\n",
    "        title = soup.title.get_text(strip=True)\n",
    "    elif soup.find('h1'):\n",
    "        title = soup.find('h1').get_text(strip=True)\n",
    "    \n",
    "    # Extract meta description\n",
    "    description = \"\"\n",
    "    meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "    if meta_desc and meta_desc.get('content'):\n",
    "        description = meta_desc['content']\n",
    "    \n",
    "    # Extract sections based on headings\n",
    "    sections = []\n",
    "    for heading in soup.find_all(['h1', 'h2', 'h3']):\n",
    "        heading_text = heading.get_text(strip=True)\n",
    "        if not heading_text or len(heading_text) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Get content after this heading until next heading\n",
    "        content_parts = []\n",
    "        for sibling in heading.find_next_siblings():\n",
    "            if sibling.name in ['h1', 'h2', 'h3']:\n",
    "                break\n",
    "            text = sibling.get_text(separator=' ', strip=True)\n",
    "            if text and len(text) > 20:\n",
    "                content_parts.append(text)\n",
    "        \n",
    "        if content_parts:\n",
    "            sections.append({\n",
    "                \"heading\": heading_text,\n",
    "                \"content\": \" \".join(content_parts)[:1000]  # Limit section content\n",
    "            })\n",
    "    \n",
    "    # Extract main content as fallback\n",
    "    main_content = \"\"\n",
    "    main_element = soup.find('main') or soup.find('article') or soup.find('body')\n",
    "    if main_element:\n",
    "        main_content = main_element.get_text(separator=' ', strip=True)\n",
    "        # Clean up whitespace\n",
    "        main_content = re.sub(r'\\s+', ' ', main_content)[:3000]  # Limit total content\n",
    "    \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "        \"sections\": sections[:10],  # Limit to 10 sections\n",
    "        \"content\": main_content\n",
    "    }\n",
    "\n",
    "\n",
    "def discover_key_pages(html: str, base_url: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Discover important internal pages from the homepage.\n",
    "    Returns a list of URLs to scrape.\n",
    "    \"\"\"\n",
    "    if not html:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    parsed_base = urlparse(base_url)\n",
    "    base_domain = parsed_base.netloc.lower()\n",
    "    \n",
    "    discovered_urls = set()\n",
    "    scored_urls = []\n",
    "    \n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        link_text = link.get_text(strip=True).lower()\n",
    "        \n",
    "        # Resolve relative URLs\n",
    "        full_url = urljoin(base_url, href)\n",
    "        parsed_url = urlparse(full_url)\n",
    "        \n",
    "        # Skip external links, anchors, and non-http\n",
    "        if parsed_url.netloc.lower() != base_domain:\n",
    "            continue\n",
    "        if not parsed_url.scheme in ['http', 'https']:\n",
    "            continue\n",
    "        if parsed_url.fragment and not parsed_url.path:\n",
    "            continue\n",
    "        \n",
    "        # Skip common non-content pages\n",
    "        skip_patterns = ['login', 'signin', 'signup', 'register', 'cart', 'checkout', \n",
    "                        'account', 'password', 'download', '.pdf', '.jpg', '.png', \n",
    "                        '.zip', 'mailto:', 'tel:', 'javascript:']\n",
    "        if any(pattern in full_url.lower() for pattern in skip_patterns):\n",
    "            continue\n",
    "        \n",
    "        # Normalize URL (remove trailing slash, query params for dedup)\n",
    "        normalized = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}\".rstrip('/')\n",
    "        \n",
    "        if normalized in discovered_urls or normalized == base_url.rstrip('/'):\n",
    "            continue\n",
    "        \n",
    "        discovered_urls.add(normalized)\n",
    "        \n",
    "        # Score the URL based on importance\n",
    "        score = 0\n",
    "        url_path = parsed_url.path.lower()\n",
    "        \n",
    "        for keyword in IMPORTANT_PAGE_KEYWORDS:\n",
    "            if keyword in url_path or keyword in link_text:\n",
    "                score += 10\n",
    "                break\n",
    "        \n",
    "        # Prefer shorter paths (usually more important)\n",
    "        path_depth = len([p for p in parsed_url.path.split('/') if p])\n",
    "        if path_depth <= 2:\n",
    "            score += 5\n",
    "        \n",
    "        # Prefer links in navigation\n",
    "        parent = link.parent\n",
    "        while parent:\n",
    "            if parent.name in ['nav', 'header']:\n",
    "                score += 3\n",
    "                break\n",
    "            parent = parent.parent\n",
    "        \n",
    "        scored_urls.append((normalized, score))\n",
    "    \n",
    "    # Sort by score descending and return top URLs\n",
    "    scored_urls.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [url for url, score in scored_urls[:MAX_PAGES_TO_SCRAPE - 1]]\n",
    "\n",
    "\n",
    "async def scrape_website(url: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Main scraping function - scrapes homepage and discovers/scrapes key pages.\n",
    "    Returns structured data for the entire website.\n",
    "    Now with: retry logic, robots.txt respect, rate limiting, better error handling.\n",
    "    \"\"\"\n",
    "    print(f\"üåê Starting smart scrape of: {url}\")\n",
    "    \n",
    "    # Normalize URL\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = 'https://' + url\n",
    "    url = url.rstrip('/')\n",
    "    \n",
    "    results = {\n",
    "        \"source_url\": url,\n",
    "        \"scraped_at\": datetime.now().isoformat(),\n",
    "        \"pages\": [],\n",
    "        \"total_pages\": 0,\n",
    "        \"success\": False,\n",
    "        \"errors\": []  # Track errors for UI feedback\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Step 0: Check robots.txt (be polite!)\n",
    "        print(\"  ü§ñ Checking robots.txt...\")\n",
    "        disallowed_paths = await check_robots_txt(session, url)\n",
    "        \n",
    "        # Step 1: Fetch homepage with retry\n",
    "        print(\"  üìÑ Fetching homepage...\")\n",
    "        _, homepage_html, homepage_error = await fetch_page_with_retry(session, url)\n",
    "        \n",
    "        if not homepage_html:\n",
    "            error_msg = f\"Failed to fetch homepage: {homepage_error}\"\n",
    "            print(f\"  ‚ùå {error_msg}\")\n",
    "            results[\"errors\"].append(error_msg)\n",
    "            return results\n",
    "        \n",
    "        # Step 2: Clean and extract homepage content\n",
    "        homepage_data = clean_html_content(homepage_html)\n",
    "        homepage_data[\"url\"] = url\n",
    "        homepage_data[\"page_type\"] = \"homepage\"\n",
    "        results[\"pages\"].append(homepage_data)\n",
    "        print(f\"  ‚úÖ Homepage: {homepage_data['title'][:50] if homepage_data['title'] else 'No title'}\")\n",
    "        \n",
    "        # Step 3: Discover key pages\n",
    "        print(\"  üîç Discovering key pages...\")\n",
    "        key_pages = discover_key_pages(homepage_html, url)\n",
    "        \n",
    "        # Filter out disallowed pages (robots.txt)\n",
    "        if disallowed_paths:\n",
    "            original_count = len(key_pages)\n",
    "            key_pages = [p for p in key_pages if is_path_allowed(p, disallowed_paths)]\n",
    "            if len(key_pages) < original_count:\n",
    "                print(f\"  üö´ Skipped {original_count - len(key_pages)} pages (robots.txt)\")\n",
    "        \n",
    "        print(f\"  üìã Found {len(key_pages)} important pages to scrape\")\n",
    "        \n",
    "        # Step 4: Scrape key pages with rate limiting\n",
    "        if key_pages:\n",
    "            print(\"  ‚ö° Scraping pages (with polite delays)...\")\n",
    "            \n",
    "            # Process in small batches to be polite\n",
    "            batch_size = 3\n",
    "            for i in range(0, len(key_pages), batch_size):\n",
    "                batch = key_pages[i:i + batch_size]\n",
    "                tasks = [fetch_page_with_retry(session, page_url) for page_url in batch]\n",
    "                page_results = await asyncio.gather(*tasks)\n",
    "                \n",
    "                for page_url, page_html, error in page_results:\n",
    "                    if page_html:\n",
    "                        page_data = clean_html_content(page_html)\n",
    "                        page_data[\"url\"] = page_url\n",
    "                        page_data[\"page_type\"] = \"subpage\"\n",
    "                        results[\"pages\"].append(page_data)\n",
    "                        print(f\"    ‚úÖ {page_url.split('/')[-1] or 'page'}: {page_data['title'][:30] if page_data['title'] else 'No title'}\")\n",
    "                    elif error:\n",
    "                        results[\"errors\"].append(f\"{page_url}: {error}\")\n",
    "                \n",
    "                # Polite delay between batches\n",
    "                if i + batch_size < len(key_pages):\n",
    "                    await asyncio.sleep(POLITE_DELAY)\n",
    "    \n",
    "    results[\"total_pages\"] = len(results[\"pages\"])\n",
    "    results[\"success\"] = results[\"total_pages\"] > 0\n",
    "    \n",
    "    # Summary\n",
    "    if results[\"errors\"]:\n",
    "        print(f\"  ‚ö†Ô∏è Completed with {len(results['errors'])} errors\")\n",
    "    print(f\"  üéâ Scraping complete: {results['total_pages']} pages extracted\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def format_scraped_content_for_context(scraped_data: Dict) -> str:\n",
    "    \"\"\"Convert scraped data into a formatted string for the chatbot context.\"\"\"\n",
    "    if not scraped_data.get(\"success\"):\n",
    "        return \"\"\n",
    "    \n",
    "    parts = []\n",
    "    parts.append(f\"=== WEBSITE CONTENT (Primary Source) ===\")\n",
    "    parts.append(f\"Source: {scraped_data['source_url']}\")\n",
    "    parts.append(f\"Pages scraped: {scraped_data['total_pages']}\")\n",
    "    parts.append(\"\")\n",
    "    \n",
    "    for page in scraped_data.get(\"pages\", []):\n",
    "        if page.get(\"title\"):\n",
    "            parts.append(f\"## {page['title']}\")\n",
    "        if page.get(\"url\"):\n",
    "            parts.append(f\"URL: {page['url']}\")\n",
    "        if page.get(\"description\"):\n",
    "            parts.append(f\"Description: {page['description']}\")\n",
    "        \n",
    "        # Add sections\n",
    "        for section in page.get(\"sections\", [])[:5]:  # Limit sections per page\n",
    "            if section.get(\"heading\"):\n",
    "                parts.append(f\"\\n### {section['heading']}\")\n",
    "            if section.get(\"content\"):\n",
    "                parts.append(section['content'][:500])\n",
    "        \n",
    "        # Add main content if no sections\n",
    "        if not page.get(\"sections\") and page.get(\"content\"):\n",
    "            parts.append(page['content'][:800])\n",
    "        \n",
    "        parts.append(\"\\n---\\n\")\n",
    "    \n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Smart Scraper loaded (Phase 3: retry, robots.txt, rate limiting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ef84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Agent Configuration\n",
    "SEARCH_INSTRUCTIONS = \"You are a research assistant. Given a search URL, you search the web for that URL and \\\n",
    "produce a concise summary of the results. The summary must 2-3 paragraphs and less than 300 \\\n",
    "words. Capture the main points. Write succintly, no need to have complete sentences or good \\\n",
    "grammar. This will be consumed by someone synthesizing a report, so it's vital you capture the \\\n",
    "essence and ignore any fluff. Do not include any additional commentary other than the summary itself.\"\n",
    "\n",
    "search_agent = Agent(\n",
    "    name=\"Search agent\",\n",
    "    instructions=SEARCH_INSTRUCTIONS,\n",
    "    tools=[WebSearchTool(search_context_size=\"low\")],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    model_settings=ModelSettings(tool_choice=\"required\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0900bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planner Agent Configuration - REDUCED FOR GAP FILLING ONLY\n",
    "HOW_MANY_SEARCHES = 5  # Reduced from 15 - only for filling gaps\n",
    "\n",
    "PLANNER_INSTRUCTIONS = f\"\"\"You are a helpful research assistant. You will be given:\n",
    "1. A URL\n",
    "2. Content already extracted from that website (PRIMARY SOURCE)\n",
    "\n",
    "Your job is to identify ONLY the gaps - information that is MISSING from the extracted content.\n",
    "Come up with {HOW_MANY_SEARCHES} targeted web searches to fill these specific gaps.\n",
    "\n",
    "DO NOT search for information that is already present in the extracted content.\n",
    "Focus on: missing contact details, pricing not found, team info gaps, specific features unclear, etc.\n",
    "\n",
    "If the extracted content is comprehensive, you can suggest fewer searches or very specific ones.\"\"\"\n",
    "\n",
    "\n",
    "class WebSearchItem(BaseModel):\n",
    "    reason: str = Field(description=\"The specific gap this search will fill.\")\n",
    "    query: str = Field(description=\"The search term to use for the web search.\")\n",
    "\n",
    "\n",
    "class WebSearchPlan(BaseModel):\n",
    "    has_significant_gaps: bool = Field(description=\"True if there are significant gaps that need web search.\")\n",
    "    searches: list[WebSearchItem] = Field(description=\"A list of web searches to fill the gaps.\")\n",
    "\n",
    "\n",
    "planner_agent = Agent(\n",
    "    name=\"PlannerAgent\",\n",
    "    instructions=PLANNER_INSTRUCTIONS,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=WebSearchPlan,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a3a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GAP DETECTION AGENT\n",
    "# ============================================================\n",
    "\n",
    "GAP_DETECTION_INSTRUCTIONS = \"\"\"You are a content analysis expert. You analyze extracted website content and determine if web searches are needed to fill gaps.\n",
    "\n",
    "Analyze the provided website content and determine:\n",
    "1. Is the content comprehensive enough for a chatbot to answer questions about this website?\n",
    "2. What specific information gaps exist (if any)?\n",
    "3. Should we run web searches to fill these gaps?\n",
    "\n",
    "Be conservative - only recommend web searches if there are CLEAR gaps like:\n",
    "- No contact information found\n",
    "- Pricing/plans mentioned but not detailed\n",
    "- Services listed but not explained\n",
    "- Team/leadership mentioned but not detailed\n",
    "- Key product features missing\n",
    "\n",
    "If the website content covers the basics (what they do, who they are, how to contact), NO web search is needed.\"\"\"\n",
    "\n",
    "\n",
    "class GapAnalysis(BaseModel):\n",
    "    has_gaps: bool = Field(description=\"True if significant information gaps exist\")\n",
    "    confidence_score: int = Field(description=\"1-10 score of how complete the extracted content is\")\n",
    "    gaps_found: list[str] = Field(description=\"List of specific gaps identified\")\n",
    "    recommended_searches: list[str] = Field(description=\"Specific search queries to fill gaps (max 5)\")\n",
    "    reasoning: str = Field(description=\"Brief explanation of the analysis\")\n",
    "\n",
    "\n",
    "gap_detection_agent = Agent(\n",
    "    name=\"GapDetectionAgent\",\n",
    "    instructions=GAP_DETECTION_INSTRUCTIONS,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=GapAnalysis,\n",
    ")\n",
    "\n",
    "\n",
    "async def analyze_content_gaps(scraped_content: str, url: str) -> GapAnalysis:\n",
    "    \"\"\"Analyze scraped content to determine if web searches are needed.\"\"\"\n",
    "    print(\"üîç Analyzing content for gaps...\")\n",
    "    \n",
    "    prompt = f\"\"\"Analyze this extracted website content and determine if web searches are needed to fill gaps.\n",
    "\n",
    "URL: {url}\n",
    "\n",
    "EXTRACTED CONTENT:\n",
    "{scraped_content[:6000]}\n",
    "\n",
    "Remember: Only recommend searches for CLEAR gaps. If basic info is present, return has_gaps=False.\"\"\"\n",
    "    \n",
    "    result = await Runner.run(gap_detection_agent, prompt)\n",
    "    \n",
    "    analysis = result.final_output\n",
    "    print(f\"  üìä Confidence: {analysis.confidence_score}/10\")\n",
    "    print(f\"  üîé Has gaps: {analysis.has_gaps}\")\n",
    "    if analysis.gaps_found:\n",
    "        print(f\"  üìã Gaps: {', '.join(analysis.gaps_found[:3])}\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "print(\"‚úÖ Gap Detection Agent loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de398adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writer Agent Configuration\n",
    "WRITER_INSTRUCTIONS = (\n",
    "    \"You are a senior researcher tasked with writing a cohesive report for a research query About website URL. \"\n",
    "    \"You will be provided with the original URL, and some initial research done by a research assistant.\\n\"\n",
    "    \"You should first come up with an outline for the Detail report that describes the structure and \"\n",
    "    \"flow of the report. Then, generate the report and return that as your final output.\\n\"\n",
    "    \"The final output should be in markdown format, and it should be lengthy and detailed. Aim \"\n",
    "    \"for 5-10 pages of content, at least 1000 words.\"\n",
    ")\n",
    "\n",
    "\n",
    "class ReportData(BaseModel):\n",
    "    short_summary: str = Field(description=\"A short 2-3 sentence summary of the findings.\")\n",
    "    markdown_report: str = Field(description=\"The final report\")\n",
    "\n",
    "\n",
    "writer_agent = Agent(\n",
    "    name=\"WriterAgent\",\n",
    "    instructions=WRITER_INSTRUCTIONS,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=ReportData,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63db11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name Extractor Agent Configuration\n",
    "NAME_AGENT_INSTRUCTIONS = (\n",
    "    \"You analyze the provided text and extract a single concise name \"\n",
    "    \"that best represents the main subject (e.g., site/company/person/product). \"\n",
    "    \"If a URL is provided, prefer the name associated with that URL. \"\n",
    "    \"Return only the name, no extra words.\"\n",
    ")\n",
    "\n",
    "\n",
    "class NameExtraction(BaseModel):\n",
    "    name: str = Field(description=\"The extracted name from the text\")\n",
    "\n",
    "\n",
    "name_extractor = Agent(\n",
    "    name=\"NameExtractor\",\n",
    "    instructions=NAME_AGENT_INSTRUCTIONS,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    output_type=NameExtraction,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a640ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Research Functions (Updated for new workflow)\n",
    "\n",
    "async def plan_gap_searches(url: str, scraped_content: str):\n",
    "    \"\"\"Use the planner_agent to plan targeted searches based on gaps in scraped content.\"\"\"\n",
    "    print(\"Planning targeted searches for gaps...\")\n",
    "    prompt = f\"\"\"URL: {url}\n",
    "\n",
    "ALREADY EXTRACTED CONTENT (PRIMARY SOURCE):\n",
    "{scraped_content[:4000]}\n",
    "\n",
    "Based on the above content, identify gaps and suggest {HOW_MANY_SEARCHES} specific searches to fill them.\n",
    "If content is comprehensive, suggest fewer searches.\"\"\"\n",
    "    \n",
    "    result = await Runner.run(planner_agent, prompt)\n",
    "    print(f\"Will perform {len(result.final_output.searches)} gap-filling searches\")\n",
    "    return result.final_output\n",
    "\n",
    "\n",
    "async def search(item: WebSearchItem):\n",
    "    \"\"\"Use the search agent to run a web search for each item in the search plan\"\"\"\n",
    "    input = f\"Search term: {item.query}\\nReason for searching: {item.reason}\"\n",
    "    result = await Runner.run(search_agent, input)\n",
    "    return result.final_output\n",
    "\n",
    "\n",
    "async def perform_searches(search_plan: WebSearchPlan):\n",
    "    \"\"\"Call search() for each item in the search plan\"\"\"\n",
    "    if not search_plan.searches:\n",
    "        print(\"No searches needed\")\n",
    "        return []\n",
    "    print(f\"Searching ({len(search_plan.searches)} queries)...\")\n",
    "    tasks = [asyncio.create_task(search(item)) for item in search_plan.searches]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    print(\"Finished searching\")\n",
    "    return results\n",
    "\n",
    "\n",
    "async def extract_name_from_text(text: str, url: str = \"\") -> str:\n",
    "    \"\"\"Extract the name from the text content\"\"\"\n",
    "    prompt = (\n",
    "        f\"Text to analyze:\\n{text[:2000]}\\n\\n\"\n",
    "        f\"Original URL: {url}\\n\\n\"\n",
    "        \"Return only the best fitting name for this website/company/organization.\"\n",
    "    )\n",
    "    result = await Runner.run(name_extractor, prompt)\n",
    "    return (result.final_output.name or \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9475289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# JSON KNOWLEDGE BASE - Storage & Caching\n",
    "# ============================================================\n",
    "\n",
    "def get_cache_path(url: str) -> str:\n",
    "    \"\"\"Get the cache file path for a given URL.\"\"\"\n",
    "    url_hash = hashlib.md5(url.encode()).hexdigest()[:12]\n",
    "    domain = urlparse(url).netloc.replace(\"www.\", \"\").replace(\".\", \"_\")\n",
    "    return f\"knowledge_files/{domain}_{url_hash}.json\"\n",
    "\n",
    "\n",
    "def is_cached(url: str) -> bool:\n",
    "    \"\"\"Check if knowledge for a URL is already cached.\"\"\"\n",
    "    cache_path = get_cache_path(url)\n",
    "    return os.path.exists(cache_path)\n",
    "\n",
    "\n",
    "def get_cached_knowledge(url: str) -> Dict | None:\n",
    "    \"\"\"Load cached knowledge if available. Returns None if not cached.\"\"\"\n",
    "    cache_path = get_cache_path(url)\n",
    "    if os.path.exists(cache_path):\n",
    "        try:\n",
    "            with open(cache_path, 'r', encoding='utf-8') as f:\n",
    "                knowledge = json.load(f)\n",
    "            print(f\"üìÇ Loaded from cache: {cache_path}\")\n",
    "            return knowledge\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Cache read error: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_knowledge_json(url: str, scraped_data: Dict, web_search_results: List = None, name: str = \"\") -> Dict:\n",
    "    \"\"\"Create a structured JSON knowledge base from all sources.\"\"\"\n",
    "    knowledge = {\n",
    "        \"metadata\": {\n",
    "            \"url\": url,\n",
    "            \"name\": name,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"pages_scraped\": scraped_data.get(\"total_pages\", 0),\n",
    "            \"has_web_search_supplement\": bool(web_search_results),\n",
    "        },\n",
    "        \"primary_content\": {\n",
    "            \"source\": \"website_scraping\",\n",
    "            \"reliability\": \"high\",\n",
    "            \"pages\": scraped_data.get(\"pages\", [])\n",
    "        },\n",
    "        \"secondary_content\": {\n",
    "            \"source\": \"web_search\",\n",
    "            \"reliability\": \"medium\",\n",
    "            \"searches\": []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add web search results if available\n",
    "    if web_search_results:\n",
    "        for i, result in enumerate(web_search_results):\n",
    "            knowledge[\"secondary_content\"][\"searches\"].append({\n",
    "                \"index\": i + 1,\n",
    "                \"result\": str(result)[:1000]\n",
    "            })\n",
    "    \n",
    "    return knowledge\n",
    "\n",
    "\n",
    "def save_knowledge_json(knowledge: Dict, url: str) -> str:\n",
    "    \"\"\"Save knowledge JSON to file. Returns filepath.\"\"\"\n",
    "    filepath = get_cache_path(url)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(knowledge, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"üíæ Knowledge saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def load_knowledge_json(filepath: str) -> Dict:\n",
    "    \"\"\"Load knowledge from a JSON file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def knowledge_to_chatbot_context(knowledge: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Convert JSON knowledge to a formatted string for chatbot context.\n",
    "    IMPROVED: Prioritizes homepage/about content for better answers.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Metadata\n",
    "    meta = knowledge.get(\"metadata\", {})\n",
    "    parts.append(f\"=== WEBSITE INFORMATION ===\")\n",
    "    parts.append(f\"Name: {meta.get('name', 'Unknown')}\")\n",
    "    parts.append(f\"URL: {meta.get('url', '')}\")\n",
    "    parts.append(f\"Pages analyzed: {meta.get('pages_scraped', 0)}\")\n",
    "    parts.append(\"\")\n",
    "    \n",
    "    # Primary content (website scraping) - PRIORITIZE HOMEPAGE AND KEY PAGES\n",
    "    primary = knowledge.get(\"primary_content\", {})\n",
    "    pages = primary.get(\"pages\", [])\n",
    "    \n",
    "    # Separate pages by priority\n",
    "    homepage = None\n",
    "    key_pages = []  # about, contact, services, books, etc.\n",
    "    blog_pages = []  # blog posts (lower priority for context)\n",
    "    \n",
    "    key_page_keywords = ['about', 'contact', 'services', 'products', 'team', \n",
    "                         'pricing', 'faq', 'books', 'publications', 'cv', 'resume']\n",
    "    \n",
    "    for page in pages:\n",
    "        page_type = page.get(\"page_type\", \"\")\n",
    "        url_lower = page.get(\"url\", \"\").lower()\n",
    "        \n",
    "        if page_type == \"homepage\":\n",
    "            homepage = page\n",
    "        elif any(kw in url_lower for kw in key_page_keywords):\n",
    "            key_pages.append(page)\n",
    "        elif 'blog' in url_lower or '/20' in url_lower:  # blog posts often have dates\n",
    "            blog_pages.append(page)\n",
    "        else:\n",
    "            key_pages.append(page)  # Default to key pages\n",
    "    \n",
    "    parts.append(\"=== PRIMARY SOURCE (Website Content) ===\")\n",
    "    parts.append(\"[This is the most reliable information - directly from the website]\")\n",
    "    parts.append(\"\")\n",
    "    \n",
    "    # 1. HOMEPAGE FIRST (most important - give it full space)\n",
    "    if homepage:\n",
    "        parts.append(\"## HOMEPAGE (Main Information)\")\n",
    "        if homepage.get(\"title\"):\n",
    "            parts.append(f\"Title: {homepage['title']}\")\n",
    "        if homepage.get(\"description\"):\n",
    "            parts.append(f\"Description: {homepage['description']}\")\n",
    "        \n",
    "        # Include ALL sections from homepage (this is where key bio info is)\n",
    "        for section in homepage.get(\"sections\", []):\n",
    "            if section.get(\"heading\"):\n",
    "                parts.append(f\"\\n### {section['heading']}\")\n",
    "            if section.get(\"content\"):\n",
    "                parts.append(section['content'][:800])  # More space for homepage\n",
    "        \n",
    "        # Also include main content\n",
    "        if homepage.get(\"content\"):\n",
    "            parts.append(f\"\\nMain content: {homepage['content'][:1500]}\")\n",
    "        \n",
    "        parts.append(\"\\n---\\n\")\n",
    "    \n",
    "    # 2. KEY PAGES (about, contact, books, etc.)\n",
    "    for page in key_pages[:5]:  # Limit to 5 key pages\n",
    "        if page.get(\"title\"):\n",
    "            parts.append(f\"## {page['title']}\")\n",
    "        if page.get(\"description\"):\n",
    "            parts.append(f\"Description: {page['description']}\")\n",
    "        \n",
    "        for section in page.get(\"sections\", [])[:4]:\n",
    "            if section.get(\"heading\"):\n",
    "                parts.append(f\"\\n### {section['heading']}\")\n",
    "            if section.get(\"content\"):\n",
    "                parts.append(section['content'][:400])\n",
    "        \n",
    "        if not page.get(\"sections\") and page.get(\"content\"):\n",
    "            parts.append(page['content'][:600])\n",
    "        \n",
    "        parts.append(\"\\n---\\n\")\n",
    "    \n",
    "    # 3. BLOG PAGES (summaries only - less important for chatbot context)\n",
    "    if blog_pages:\n",
    "        parts.append(\"\\n## BLOG/ARTICLES (Recent posts)\")\n",
    "        for page in blog_pages[:3]:  # Only top 3 blog posts\n",
    "            title = page.get(\"title\", \"\")\n",
    "            desc = page.get(\"description\", \"\")\n",
    "            if title:\n",
    "                parts.append(f\"- {title}\")\n",
    "            if desc:\n",
    "                parts.append(f\"  {desc[:200]}\")\n",
    "        parts.append(\"\\n---\\n\")\n",
    "    \n",
    "    # Secondary content (web search)\n",
    "    secondary = knowledge.get(\"secondary_content\", {})\n",
    "    if secondary.get(\"searches\"):\n",
    "        parts.append(\"\\n=== SECONDARY SOURCE (Web Search Supplement) ===\")\n",
    "        parts.append(\"[Use this only if primary source doesn't have the answer]\")\n",
    "        parts.append(\"\")\n",
    "        \n",
    "        for search in secondary.get(\"searches\", [])[:5]:\n",
    "            parts.append(f\"Search result {search.get('index', '')}:\")\n",
    "            parts.append(search.get('result', '')[:500])\n",
    "            parts.append(\"\")\n",
    "    \n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "print(\"‚úÖ JSON Knowledge Base functions loaded (with caching)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d70fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UI HELPER FUNCTIONS (Updated for new workflow + Phase 3 Error Handling)\n",
    "# ============================================================\n",
    "\n",
    "def build_status_new(percent: float, current_step: int, selected_name: str | None = None, \n",
    "                     finished: bool = False, stats: Dict = None, from_cache: bool = False,\n",
    "                     errors: List[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Build status text with percentage, steps, and progress bar\n",
    "    Updated steps for the new scraper-first workflow\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        \"Scraping website (PRIMARY SOURCE)\",\n",
    "        \"Analyzing content gaps\",\n",
    "        \"Running targeted searches (if needed)\",\n",
    "        \"Building knowledge base\",\n",
    "        \"Extracting name & preparing chatbot\",\n",
    "    ]\n",
    "\n",
    "    # Progress bar line\n",
    "    bar_len = 24\n",
    "    filled = int(bar_len * percent / 100)\n",
    "    bar = \"‚ñà\" * filled + \"‚ñë\" * (bar_len - filled)\n",
    "\n",
    "    # Step list with icons\n",
    "    lines = []\n",
    "    for i, label in enumerate(steps):\n",
    "        if finished or i < current_step:\n",
    "            icon = \"‚úÖ\"\n",
    "        elif i == current_step:\n",
    "            icon = \"üîÑ\"\n",
    "        else:\n",
    "            icon = \"‚è≥\"\n",
    "        lines.append(f\"- {icon} Step {i+1}: {label}\")\n",
    "\n",
    "    text = f\"### Progress: {percent:.0f}%\\n\\n`{bar}`\\n\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "    # Add stats if available\n",
    "    if stats:\n",
    "        text += f\"\\n\\nüìä **Stats:**\"\n",
    "        if \"pages_scraped\" in stats:\n",
    "            text += f\"\\n- Pages scraped: {stats['pages_scraped']}\"\n",
    "        if \"searches_run\" in stats:\n",
    "            text += f\"\\n- Web searches: {stats['searches_run']}\"\n",
    "        if \"gaps_found\" in stats:\n",
    "            text += f\"\\n- Gaps filled: {stats['gaps_found']}\"\n",
    "\n",
    "    # Show errors if any (Phase 3 enhancement)\n",
    "    if errors and len(errors) > 0:\n",
    "        text += f\"\\n\\n‚ö†Ô∏è **Warnings ({len(errors)}):**\"\n",
    "        for err in errors[:3]:  # Show max 3 errors\n",
    "            text += f\"\\n- {err[:60]}...\"\n",
    "\n",
    "    if finished:\n",
    "        if from_cache:\n",
    "            text += f\"\\n\\n‚ö° **Loaded from cache** (instant!)\"\n",
    "        if selected_name:\n",
    "            text += f\"\\n\\n**Selected name:** `{selected_name}`\"\n",
    "        text += \"\\n\\nü§ñ Chatbot is ready. Ask your questions below.\"\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def build_error_status(error_type: str, details: str = \"\") -> str:\n",
    "    \"\"\"Build a user-friendly error status message.\"\"\"\n",
    "    error_messages = {\n",
    "        \"invalid_url\": \"‚ùå **Invalid URL**\\n\\nPlease enter a valid website URL (e.g., https://example.com)\",\n",
    "        \"connection_failed\": f\"‚ùå **Connection Failed**\\n\\nCouldn't connect to the website. Please check:\\n- The URL is correct\\n- The website is online\\n- Your internet connection\\n\\n{details}\",\n",
    "        \"scrape_failed\": f\"‚ùå **Scraping Failed**\\n\\nCouldn't extract content from this website.\\n\\nPossible reasons:\\n- Website blocks automated access\\n- JavaScript-heavy site (not fully supported)\\n- robots.txt restrictions\\n\\n{details}\",\n",
    "        \"api_error\": f\"‚ùå **API Error**\\n\\nAn error occurred while processing.\\n\\n{details}\\n\\nPlease try again.\",\n",
    "        \"timeout\": \"‚ùå **Timeout**\\n\\nThe request took too long. The website might be slow or unresponsive.\\n\\nTry again or use a different URL.\",\n",
    "    }\n",
    "    return error_messages.get(error_type, f\"‚ùå **Error**\\n\\n{details}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NEW MAIN RESEARCH PIPELINE (Scraper-First Approach with Caching + Error Handling)\n",
    "# ============================================================\n",
    "\n",
    "async def run_full_research_new(url: str, force_refresh: bool = False, progress=gr.Progress()):\n",
    "    \"\"\"\n",
    "    NEW workflow: Scrape first, then fill gaps with targeted searches.\n",
    "    With caching support and improved error handling (Phase 3).\n",
    "    \"\"\"\n",
    "    stats = {\"pages_scraped\": 0, \"searches_run\": 0, \"gaps_found\": 0}\n",
    "    errors = []  # Track errors for UI feedback\n",
    "    \n",
    "    # ===== Check Cache First =====\n",
    "    if not force_refresh and is_cached(url):\n",
    "        progress(0.5, desc=\"Loading from cache...\")\n",
    "        \n",
    "        cached_knowledge = get_cached_knowledge(url)\n",
    "        if cached_knowledge:\n",
    "            progress(0.9, desc=\"Preparing chatbot from cache...\")\n",
    "            \n",
    "            # Extract name from cached data\n",
    "            raw_name = cached_knowledge.get(\"metadata\", {}).get(\"name\", \"the site\")\n",
    "            stats[\"pages_scraped\"] = cached_knowledge.get(\"metadata\", {}).get(\"pages_scraped\", 0)\n",
    "            \n",
    "            chatbot_context = knowledge_to_chatbot_context(cached_knowledge)\n",
    "            \n",
    "            # Build system prompt\n",
    "            system_prompt = f\"\"\"You are an AI assistant for {raw_name} ({url}).\n",
    "\n",
    "RULES:\n",
    "1. Answer ONLY from the knowledge base below - never make things up.\n",
    "2. Search the knowledge carefully before saying \"I don't know\".\n",
    "3. For bio questions, check the HOMEPAGE section first.\n",
    "4. Give partial info if available (e.g., \"The site mentions X but not Y...\").\n",
    "5. Keep answers concise and helpful.\n",
    "\n",
    "=== KNOWLEDGE BASE ===\n",
    "\n",
    "{chatbot_context[:10000]}\n",
    "\n",
    "=== END ===\n",
    "\"\"\"\n",
    "            progress(1.0, desc=\"Done (from cache)!\")\n",
    "            status_text = build_status_new(100, current_step=4, selected_name=raw_name, \n",
    "                                           finished=True, stats=stats, from_cache=True)\n",
    "            \n",
    "            msg_update = gr.update(interactive=True, placeholder=\"Ask anything about the website...\")\n",
    "            send_btn_update = gr.update(interactive=True)\n",
    "            \n",
    "            return status_text, system_prompt, raw_name, [], msg_update, send_btn_update\n",
    "    \n",
    "    # ===== Step 1: Scrape Website (PRIMARY SOURCE) =====\n",
    "    progress(0.05, desc=\"Scraping website...\")\n",
    "    status_text = build_status_new(5, current_step=0, stats=stats)\n",
    "    \n",
    "    try:\n",
    "        scraped_data = await scrape_website(url)\n",
    "        stats[\"pages_scraped\"] = scraped_data.get(\"total_pages\", 0)\n",
    "        errors.extend(scraped_data.get(\"errors\", []))  # Collect scraping errors\n",
    "        \n",
    "        if not scraped_data.get(\"success\"):\n",
    "            print(\"‚ö†Ô∏è Scraping failed, falling back to web search only...\")\n",
    "            scraped_content = \"\"\n",
    "        else:\n",
    "            scraped_content = format_scraped_content_for_context(scraped_data)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Scraping error: {e}\")\n",
    "        scraped_content = \"\"\n",
    "        scraped_data = {\"pages\": [], \"total_pages\": 0, \"success\": False}\n",
    "        errors.append(f\"Scraping error: {str(e)[:50]}\")\n",
    "    \n",
    "    # ===== Step 2: Analyze Content Gaps =====\n",
    "    progress(0.25, desc=\"Analyzing content gaps...\")\n",
    "    status_text = build_status_new(25, current_step=1, stats=stats, errors=errors)\n",
    "    \n",
    "    search_results = []\n",
    "    \n",
    "    if scraped_content:\n",
    "        try:\n",
    "            gap_analysis = await analyze_content_gaps(scraped_content, url)\n",
    "            stats[\"gaps_found\"] = len(gap_analysis.gaps_found)\n",
    "            \n",
    "            # ===== Step 3: Run Targeted Searches (if needed) =====\n",
    "            if gap_analysis.has_gaps and gap_analysis.confidence_score < 7:\n",
    "                progress(0.45, desc=\"Running targeted searches...\")\n",
    "                status_text = build_status_new(45, current_step=2, stats=stats, errors=errors)\n",
    "                \n",
    "                search_items = []\n",
    "                for query in gap_analysis.recommended_searches[:HOW_MANY_SEARCHES]:\n",
    "                    search_items.append(WebSearchItem(\n",
    "                        reason=f\"Filling gap: {query}\",\n",
    "                        query=f\"{url} {query}\"\n",
    "                    ))\n",
    "                \n",
    "                if search_items:\n",
    "                    search_plan = WebSearchPlan(has_significant_gaps=True, searches=search_items)\n",
    "                    search_results = await perform_searches(search_plan)\n",
    "                    stats[\"searches_run\"] = len(search_results)\n",
    "            else:\n",
    "                progress(0.45, desc=\"Content comprehensive, skipping web search\")\n",
    "                status_text = build_status_new(45, current_step=2, stats=stats, errors=errors)\n",
    "                print(\"‚úÖ Content is comprehensive, no web search needed!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Gap analysis error: {e}\")\n",
    "            errors.append(f\"Analysis error: {str(e)[:50]}\")\n",
    "    else:\n",
    "        # Fallback to web search when scraping fails\n",
    "        progress(0.45, desc=\"Fallback: Running web searches...\")\n",
    "        status_text = build_status_new(45, current_step=2, stats=stats, errors=errors)\n",
    "        \n",
    "        try:\n",
    "            search_plan = await plan_gap_searches(url, \"\")\n",
    "            search_results = await perform_searches(search_plan)\n",
    "            stats[\"searches_run\"] = len(search_results)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Search error: {e}\")\n",
    "            errors.append(f\"Search error: {str(e)[:50]}\")\n",
    "    \n",
    "    # Check if we have any content at all\n",
    "    if not scraped_content and not search_results:\n",
    "        error_status = build_error_status(\"scrape_failed\", \n",
    "            f\"Could not extract content from {url}. Try a different URL or check if the site is accessible.\")\n",
    "        return (\n",
    "            error_status,\n",
    "            \"\",\n",
    "            \"the site\",\n",
    "            [],\n",
    "            gr.update(interactive=False),\n",
    "            gr.update(interactive=False),\n",
    "        )\n",
    "    \n",
    "    # ===== Step 4: Build Knowledge Base =====\n",
    "    progress(0.70, desc=\"Building knowledge base...\")\n",
    "    status_text = build_status_new(70, current_step=3, stats=stats, errors=errors)\n",
    "    \n",
    "    try:\n",
    "        name_source = scraped_content[:2000] if scraped_content else str(search_results)[:2000]\n",
    "        raw_name = await extract_name_from_text(name_source, url)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Name extraction error: {e}\")\n",
    "        raw_name = \"\"\n",
    "    \n",
    "    if not raw_name:\n",
    "        try:\n",
    "            host = urlparse(url).netloc\n",
    "            raw_name = host.replace(\"www.\", \"\").split('.')[0].title() or \"the site\"\n",
    "        except Exception:\n",
    "            raw_name = \"the site\"\n",
    "    \n",
    "    knowledge = create_knowledge_json(url, scraped_data, search_results, raw_name)\n",
    "    \n",
    "    try:\n",
    "        knowledge_filepath = save_knowledge_json(knowledge, url)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not save cache: {e}\")\n",
    "        errors.append(f\"Cache save failed: {str(e)[:30]}\")\n",
    "    \n",
    "    # ===== Step 5: Prepare Chatbot =====\n",
    "    progress(0.90, desc=\"Preparing chatbot...\")\n",
    "    status_text = build_status_new(90, current_step=4, stats=stats, errors=errors)\n",
    "    \n",
    "    chatbot_context = knowledge_to_chatbot_context(knowledge)\n",
    "    \n",
    "    # IMPROVED SYSTEM PROMPT - Concise for faster responses\n",
    "    system_prompt = f\"\"\"You are an AI assistant for {raw_name} ({url}).\n",
    "\n",
    "RULES:\n",
    "1. Answer ONLY from the knowledge base below - never make things up.\n",
    "2. Search the knowledge carefully before saying \"I don't know\".\n",
    "3. For bio questions, check the HOMEPAGE section first.\n",
    "4. Give partial info if available (e.g., \"The site mentions X but not Y...\").\n",
    "5. Keep answers concise and helpful.\n",
    "\n",
    "=== KNOWLEDGE BASE ===\n",
    "\n",
    "{chatbot_context[:10000]}\n",
    "\n",
    "=== END ===\n",
    "\"\"\"\n",
    "    \n",
    "    progress(1.0, desc=\"Done!\")\n",
    "    status_text = build_status_new(100, current_step=4, selected_name=raw_name, \n",
    "                                   finished=True, stats=stats, errors=errors)\n",
    "    \n",
    "    # Return empty list for chatbot and update other components\n",
    "    msg_update = gr.update(interactive=True, placeholder=\"Ask anything about the website...\")\n",
    "    send_btn_update = gr.update(interactive=True)\n",
    "    \n",
    "    # Return empty list directly for chatbot (not gr.update)\n",
    "    return status_text, system_prompt, raw_name, [], msg_update, send_btn_update\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CHATBOT FUNCTIONS - Fixed response extraction\n",
    "# ============================================================\n",
    "\n",
    "def chat_fn(message, history, system_prompt, name):\n",
    "    \"\"\"Handle chatbot conversation - Gradio 6.x uses dict format\"\"\"\n",
    "    # Ensure history is a list\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    if not message or not message.strip():\n",
    "        return \"\", history\n",
    "    \n",
    "    if not system_prompt:\n",
    "        return \"\", history + [\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "            {\"role\": \"assistant\", \"content\": \"‚ö†Ô∏è Please generate a chatbot first! Enter a URL above and click 'Generate Chatbot'.\"}\n",
    "        ]\n",
    "\n",
    "    # Build messages for OpenAI API\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    # Convert history to OpenAI format\n",
    "    for msg in history:\n",
    "        if isinstance(msg, dict) and \"role\" in msg and \"content\" in msg:\n",
    "            # Ensure content is a string (fix for malformed responses)\n",
    "            content = msg[\"content\"]\n",
    "            if isinstance(content, list):\n",
    "                # Handle case where content is a list of dicts like [{'text': '...', 'type': 'text'}]\n",
    "                content = \" \".join(\n",
    "                    item.get(\"text\", str(item)) if isinstance(item, dict) else str(item)\n",
    "                    for item in content\n",
    "                )\n",
    "            messages.append({\"role\": msg[\"role\"], \"content\": str(content)})\n",
    "\n",
    "    # Add new user message\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # Call OpenAI with error handling\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        \n",
    "        # Extract answer - handle different response formats\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Ensure answer is a plain string\n",
    "        if answer is None:\n",
    "            answer = \"I couldn't generate a response. Please try again.\"\n",
    "        elif isinstance(answer, list):\n",
    "            # Handle list format response\n",
    "            answer = \" \".join(\n",
    "                item.get(\"text\", str(item)) if isinstance(item, dict) else str(item)\n",
    "                for item in answer\n",
    "            )\n",
    "        else:\n",
    "            answer = str(answer)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Chat error: {e}\")\n",
    "        answer = f\"‚ö†Ô∏è Sorry, there was an error generating a response. Please try again.\\n\\nError: {str(e)[:100]}\"\n",
    "\n",
    "    # Return in Gradio 6.x format\n",
    "    return \"\", history + [\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "\n",
    "\n",
    "async def handle_run_research(url, force_refresh, progress=gr.Progress()):\n",
    "    \"\"\"Handle research button click - uses the NEW workflow with caching and error handling\"\"\"\n",
    "    if not url or not url.strip():\n",
    "        return (\n",
    "            build_error_status(\"invalid_url\"),\n",
    "            \"\",\n",
    "            \"the site\",\n",
    "            [],\n",
    "            gr.update(interactive=False),\n",
    "            gr.update(interactive=False),\n",
    "        )\n",
    "    \n",
    "    # Basic URL validation\n",
    "    url = url.strip()\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = 'https://' + url\n",
    "    \n",
    "    # Show cache status\n",
    "    if not force_refresh and is_cached(url):\n",
    "        print(f\"üìÇ Cache found for {url}, loading instantly...\")\n",
    "    elif force_refresh and is_cached(url):\n",
    "        print(f\"üîÑ Force refresh requested, re-processing {url}...\")\n",
    "\n",
    "    try:\n",
    "        result = await run_full_research_new(url, force_refresh=force_refresh, progress=progress)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Research error: {e}\")\n",
    "        return (\n",
    "            build_error_status(\"api_error\", str(e)[:200]),\n",
    "            \"\",\n",
    "            \"the site\",\n",
    "            [],\n",
    "            gr.update(interactive=False),\n",
    "            gr.update(interactive=False),\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# GRADIO UI - Gradio 6.x compatible with Caching & Refresh & Error Handling\n",
    "# ============================================================\n",
    "\n",
    "with gr.Blocks(title=\"ChatSMITH - Website to Chatbot\") as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    ## ü§ñ ChatSMITH - Website to Chatbot Generator\n",
    "    \n",
    "    **New & Improved!** Uses smart website scraping as the PRIMARY source for faster, more accurate results.\n",
    "    \n",
    "    **How to use:** Enter a website URL ‚Üí Click \"Generate Chatbot\" ‚Üí Wait for processing ‚Üí Chat!\n",
    "    \n",
    "    üí° **Tip:** Previously processed websites load instantly from cache!\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        url_in = gr.Textbox(\n",
    "            label=\"Website URL\", \n",
    "            placeholder=\"https://example.com\",\n",
    "            scale=4\n",
    "        )\n",
    "        force_refresh = gr.Checkbox(\n",
    "            label=\"üîÑ Force Refresh\",\n",
    "            value=False,\n",
    "            info=\"Re-scrape the website even if cached\"\n",
    "        )\n",
    "        run_btn = gr.Button(\"üöÄ Generate Chatbot\", variant=\"primary\", scale=1)\n",
    "\n",
    "    status_box = gr.Markdown(\"‚û°Ô∏è Enter a URL and click **Generate Chatbot** to start.\")\n",
    "\n",
    "    # Hidden state\n",
    "    system_prompt_state = gr.State(\"\")\n",
    "    name_state = gr.State(\"the site\")\n",
    "\n",
    "    gr.Markdown(\"---\")\n",
    "    gr.Markdown(\"### üí¨ Chat with the website\")\n",
    "\n",
    "    # Chatbot - Gradio 6.x uses messages format by default\n",
    "    chatbot = gr.Chatbot(label=\"Chat\", height=400, value=[])\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(\n",
    "            label=\"Your question\", \n",
    "            placeholder=\"Generate a chatbot first, then ask questions here...\", \n",
    "            scale=4,\n",
    "            interactive=False\n",
    "        )\n",
    "        send_btn = gr.Button(\"Send\", scale=1, interactive=False)\n",
    "\n",
    "    # Event handlers\n",
    "    run_btn.click(\n",
    "        fn=handle_run_research,\n",
    "        inputs=[url_in, force_refresh],\n",
    "        outputs=[status_box, system_prompt_state, name_state, chatbot, msg, send_btn],\n",
    "    )\n",
    "\n",
    "    send_btn.click(\n",
    "        fn=chat_fn,\n",
    "        inputs=[msg, chatbot, system_prompt_state, name_state],\n",
    "        outputs=[msg, chatbot],\n",
    "    )\n",
    "\n",
    "    msg.submit(\n",
    "        fn=chat_fn,\n",
    "        inputs=[msg, chatbot, system_prompt_state, name_state],\n",
    "        outputs=[msg, chatbot],\n",
    "    )\n",
    "\n",
    "# Launch\n",
    "demo.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eda951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
