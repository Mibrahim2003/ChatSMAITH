{
  "metadata": {
    "url": "https://sebastianraschka.com/",
    "name": "Sebastian Raschka",
    "created_at": "2025-12-04T22:27:52.448905",
    "pages_scraped": 9,
    "has_web_search_supplement": false
  },
  "primary_content": {
    "source": "website_scraping",
    "reliability": "high",
    "pages": [
      {
        "title": "Sebastian Raschka | Sebastian Raschka, PhD",
        "description": "I’m an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.",
        "sections": [
          {
            "heading": "Hello, I'm Sebastian Raschka, PhD",
            "content": "I am an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, including roles as senior engineer at Lightning AI and as a statistics professor at the University of Wisconsin-Madison. I am also the author of Build a Large Language Model (From Scratch) . My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.\n            (For my most up-to-date CV details, please visit my LinkedIn profile .)"
          },
          {
            "heading": "A Technical Tour of the DeepSeek Models from V3 to V3.2",
            "content": "Similar to DeepSeek V3, the team released their new flagship model over a major US holiday weekend. Given DeepSeek V3.2's really good performance (on GPT-5 and Gemini 3.0 Pro) level, and the fact t..."
          },
          {
            "heading": "Recommendations for Getting the Most Out of a Technical Book",
            "content": "This short article compiles a few notes I previously shared when readers ask how to get the most out of my building large language model from scratch books. I follow a similar approach when I read ..."
          },
          {
            "heading": "Beyond Standard LLMs",
            "content": "After I shared my Big LLM Architecture Comparison a few months ago, which focused on the main transformer-based LLMs, I received a lot of questions with respect to what I think about alternative ap..."
          },
          {
            "heading": "DGX Spark and Mac Mini for Local PyTorch Development",
            "content": "The DGX Spark for local LLM inferencing and fine-tuning was a pretty popular discussion topic recently. I got to play with one myself, primarily working with and on LLMs in PyTorch, and collected s..."
          },
          {
            "heading": "Understanding the 4 Main Approaches to LLM Evaluation (From Scratch)",
            "content": "Multiple-Choice Benchmarks, Verifiers, Leaderboards, and LLM Judges with Code Examples"
          }
        ],
        "content": "A Technical Tour of the DeepSeek Models from V3 to V3.2 Dec 3, 2025 Similar to DeepSeek V3, the team released their new flagship model over a major US holiday weekend. Given DeepSeek V3.2's really good performance (on GPT-5 and Gemini 3.0 Pro) level, and the fact t... Recommendations for Getting the Most Out of a Technical Book Nov 12, 2025 This short article compiles a few notes I previously shared when readers ask how to get the most out of my building large language model from scratch books. I follow a similar approach when I read ... Beyond Standard LLMs Nov 4, 2025 After I shared my Big LLM Architecture Comparison a few months ago, which focused on the main transformer-based LLMs, I received a lot of questions with respect to what I think about alternative ap... DGX Spark and Mac Mini for Local PyTorch Development Oct 29, 2025 The DGX Spark for local LLM inferencing and fine-tuning was a pretty popular discussion topic recently. I got to play with one myself, primarily working with and on LLMs in PyTorch, and collected s... Understanding the 4 Main Approaches to LLM Evaluation (From Scratch) Oct 5, 2025 Multiple-Choice Benchmarks, Verifiers, Leaderboards, and LLM Judges with Code Examples",
        "url": "https://sebastianraschka.com",
        "page_type": "homepage"
      },
      {
        "title": "Sebastian Raschka, PhD",
        "description": "",
        "sections": [],
        "content": "Sebastian Raschka, PhD I'm an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations. https://sebastianraschka.com/ Thu, 04 Dec 2025 03:49:11 +0000 Thu, 04 Dec 2025 03:49:11 +0000 A Technical Tour of the DeepSeek Models from V3 to V3.2 Similar to DeepSeek V3, the team released their new flagship model over a major US holiday weekend. Given DeepSeek V3.2's really good performance (on GPT-5 and Gemini 3.0 Pro) level, and the fact that it's also available as an open-weight model, it's definitely Wed, 03 Dec 2025 00:06:00 +0000 https://sebastianraschka.com/blog/2025/technical-deepseek.html https://sebastianraschka.com/blog/2025/technical-deepseek.html Deep Learning, Machine AI, LLM Recommendations for Getting the Most Out of a Technical Book This short article compiles a few notes I previously shared when readers ask how to get the most out of my building large language model from scratch books. I follow a similar approach when I read technical books myself. It is not meant as a universal recipe, but it may be a helpful starting point. For this particular book, I strongly suggest reading it in order since each chapter depends on the previous one. And for each chapter, I recommend the following steps. Wed, 12 Nov 2025 00:08:00 +0000 https://sebastianraschka.com/blog/2025/reading-books.html https://sebastianraschka.com/blog/2025/reading-books.html Deep Learning, Machine AI, LLM Beyond Standard LLMs After I shared my Big LLM Architecture Comparison a few months ago, which focused on the main transformer-based LLMs, I received a lot of questions with respect to what I think about alternative approaches. (I also recently gave a short talk about that at the PyTorch Conference 2025, where I also promised attendees to follow up with a write-up of these alternative approaches). So here it is! Tue, 04 Nov 2025 00:08:00 +0000 https://sebastianraschka.com/blog/2025/beyond-standard-llms.html https://sebastianraschka.com/blog/2025/beyond-standard-llms.html Deep Learning, Machine AI, LLM DGX Spark and Mac Mini for Local PyTorch Development The DGX Spark for local LLM inferencing and fine-tuning was a pretty popular discussion topic recently. I got to play with one myself, primarily working with and on LLMs in PyTorch, and collected some benchmarks and takeaways. Wed, 29 Oct 2025 00:06:00 +0000 https://sebastianraschka.com/blog/2025/dgx-impressions.html https://sebastianraschka.com/blog/2025/dgx-impressions.html Deep Learning, Machine AI, LLM Understanding the 4 Main Approaches to LLM Evaluation (From Scratch) Multiple-Choice Benchmarks, Verifiers, Leaderboards, and LLM Judges with Code Examples Sun, 05 Oct 2025 00:06:00 +0000 https://sebastianraschka.com/blog/2025/llm-evaluation",
        "url": "https://sebastianraschka.com/rss_feed.xml",
        "page_type": "subpage"
      },
      {
        "title": "Blog and Notes | Sebastian Raschka, PhD",
        "description": "I’m an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.",
        "sections": [
          {
            "heading": "2025",
            "content": "Dec 3, 2025 A Technical Tour of the DeepSeek Models from V3 to V3.2 Understanding How DeepSeek's Flagship Open-Weight Models Evolved Similar to DeepSeek V3, the team released their new flagship model over a major US holiday weekend. Given DeepSeek V3.2's really good performance (on GPT-5 and Gemini 3.0 Pro) level, and the fact t... Nov 12, 2025 Recommendations for Getting the Most Out of a Technical Book This short article compiles a few notes I previously shared when readers ask how to get the most out of my building large language model from scratch books. I follow a similar approach when I read ... Nov 4, 2025 Beyond Standard LLMs Linear Attention Hybrids, Text Diffusion, Code World Models, and Small Recursive Transformers After I shared my Big LLM Architecture Comparison a few months ago, which focused on the main transformer-based LLMs, I received a lot of questions with respect to what I think about alternative ap... Oct 29, 2025 DGX Spark and Mac Mini for Local PyTorch Developme"
          },
          {
            "heading": "2024",
            "content": "Dec 29, 2024 LLM Research Papers: The 2024 List I want to share my running bookmark list of many fascinating (mostly LLM-related) papers I stumbled upon in 2024. It's just a list, but maybe it will come in handy for those who are interested in f... Nov 3, 2024 Understanding Multimodal LLMs An Introduction to the Main Techniques and Latest Models There has been a lot of new research on the multimodal LLM front, including the latest Llama 3.2 vision models, which employ diverse architectural strategies to integrate various data types like te... Sep 21, 2024 Building A GPT-Style LLM Classifier From Scratch Finetuning a GPT Model for Spam Classification This article shows you how to transform pretrained large language models (LLMs) into strong text classifiers. But why focus on classification? First, finetuning a pretrained model for classificatio... Sep 1, 2024 Building LLMs from the Ground Up: A 3-hour Coding Workshop This tutorial is aimed at coders interested in understanding the build"
          },
          {
            "heading": "2023",
            "content": "Sep 15, 2023 Optimizing LLMs From a Dataset Perspective This article focuses on improving the modeling performance of LLMs by finetuning them using carefully curated datasets. Specifically, this article highlights strategies that involve modifying, util... Aug 10, 2023 The NeurIPS 2023 LLM Efficiency Challenge Starter Guide Large language models (LLMs) offer one of the most interesting opportunities for developing more efficient training methods. A few weeks ago, the NeurIPS 2023 LLM Efficiency Challenge launched to f... Jul 1, 2023 Optimizing Memory Usage for Training LLMs and Vision Transformers in PyTorch Peak memory consumption is a common bottleneck when training deep learning models such as vision transformers and LLMs. This article provides a series of techniques that can lower memory consumptio... Jun 14, 2023 Finetuning Falcon LLMs More Efficiently With LoRA and Adapters Finetuning allows us to adapt pretrained LLMs in a cost-efficient manner. But which method should we use? T"
          },
          {
            "heading": "2022",
            "content": "Oct 15, 2022 Ahead Of AI, And What's Next? About monthly machine learning musings, and other things I am currently workin on ... Jul 24, 2022 A Short Chronology Of Deep Learning For Tabular Data Occasionally, I share research papers proposing new deep learning approaches for tabular data on social media, which is typically an excellent discussion starter. Often, people ask for additional m... Jul 5, 2022 No, We Don't Have to Choose Batch Sizes As Powers Of 2 Regarding neural network training, I think we are all guilty of doing this: we choose our batch sizes as powers of 2, that is, 64, 128, 256, 512, 1024, and so forth. There are some valid theoretica... Jun 30, 2022 Sharing Deep Learning Research Models with Lightning Part 2: Leveraging the Cloud In this article, we will take deploy a Super Resolution App on the cloud using lightning.ai. The primary goal here is to see how easy it is to create and share a research demo. However, the cloud i... Jun 17, 2022 Sharing Deep Learning Resea"
          },
          {
            "heading": "2021",
            "content": "Dec 29, 2021 Introduction to Machine Learning -- Video Lectures about Python Basics, Tree-based Methods, Model Evaluation, and Feature Selection About half a year ago, I organized all my deep learning-related videos in a handy blog post to have everything in one place. Since many people liked this post, and because I like to use my winter b... Jul 9, 2021 Introduction to Deep Learning -- 170 Video Lectures from Adaptive Linear Neurons to Zero-shot Classification with Transformers I just sat down this morning and organized all deep learning related videos I recorded in 2021. I am sure this will be a useful reference for my future self, but I am also hoping it might be useful... Feb 11, 2021 Datasets for Machine Learning and Deep Learning --  Some of the Best Places to Explore With the semester being in full swing, I recently shared this set of dataset repositories with my deep learning class. However, I thought that beyond using this list for finding inspiration for int... Jan 21, 2021 "
          },
          {
            "heading": "2020",
            "content": "Sep 27, 2020 Scientific Computing in Python: Introduction to NumPy and Matplotlib -- Including Video Tutorials Since many students in my Stat 451 (Introduction to Machine Learning and Statistical Pattern Classification) class are relatively new to Python and NumPy, I was recently devoting a lecture to the l... Aug 26, 2020 Interpretable Machine Learning -- Book Review and Thoughts about Linear and Logistic Regression as Interpretable Models In this blog post, I am (briefly) reviewing Christoph Molnar's *Interpretable Machine Learning Book*. Then, I am writing about two classic generalized linear models, linear and logistic regression.... Aug 5, 2020 Chapter 1: Introduction to Machine Learning and Deep Learning The first chapter (draft) of the Introduction to Deep Learning book, which is a book based on my lecture notes and slides. Jan 6, 2020 Book Review: Architects of Intelligence by Martin Ford A brief review of Martin Ford's book that features interviews with 23 of the most well-kno"
          },
          {
            "heading": "2019",
            "content": "Dec 12, 2019 What's New in the 3rd Edition A brief summary of what's new in the 3rd edition of Python Machine Learning. May 24, 2019 My First Year at UW-Madison and a Gallery of Awesome Student Projects Not too long ago, in the Summer of 2018, I was super excited to join the Department of Statistics at the University of Wisconsin-Madison after obtaining my Ph.D. after ~5 long and productive years...."
          },
          {
            "heading": "2018",
            "content": "Nov 10, 2018 Model evaluation, model selection, and algorithm selection in machine learning Part IV - Comparing the performance of machine learning models and algorithms using statistical tests and nested cross-validation This final article in the series *Model evaluation, model selection, and algorithm selection in machine learning* presents overviews of several statistical hypothesis testing approaches, with appli... Aug 2, 2018 Generating Gender-Neutral Face Images with Semi-Adversarial Neural Networks to Enhance Privacy I thought that it would be nice to have short and concise summaries of recent projects handy, to share them with a more general audience, including colleagues and students. So, I challenged myself ..."
          },
          {
            "heading": "2016",
            "content": "Oct 2, 2016 Model evaluation, model selection, and algorithm selection in machine learning Part III - Cross-validation and hyperparameter tuning Almost every machine learning algorithm comes with a large number of settings that we, the machine learning researchers and practitioners, need to specify. These tuning knobs, the so-called hyperpa... Aug 13, 2016 Model evaluation, model selection, and algorithm selection in machine learning Part II - Bootstrapping and uncertainties In this second part of this series, we will look at some advanced techniques for model evaluation and techniques to estimate the uncertainty of our estimated model performance as well as its varian... Jun 11, 2016 Model evaluation, model selection, and algorithm selection in machine learning Part I - The basics Machine learning has become a central part of our life -- as consumers, customers, and hopefully as researchers and practitioners! Whether we are applying predictive modeling techniques to our rese..."
          },
          {
            "heading": "2015",
            "content": "Sep 24, 2015 Writing 'Python Machine Learning' – A Reflection on a Journey It's been about time. I am happy to announce that \"Python Machine Learning\" was finally released today! Sure, I could just send an email around to all the people who were interested in this book. O... Aug 24, 2015 Python, Machine Learning, and Language Wars – A Highly Subjective Point of View This has really been quite a journey for me lately. And regarding the frequently asked question “Why did you choose Python for Machine Learning?” I guess it is about time to write my script. In thi... Mar 24, 2015 Single-Layer Neural Networks and Gradient Descent This article offers a brief glimpse of the history and basic concepts of machine learning. We will take a look at the first algorithmically described neural network and the gradient descent algorit... Jan 27, 2015 Principal Component Analysis in 3 Simple Steps Principal Component Analysis (PCA) is a simple yet popular and useful linear transformation technique that"
          }
        ],
        "content": "RSS Feed (Blog and Notes) Subscribe via Email (AI Magazine) 2025 Dec 3, 2025 A Technical Tour of the DeepSeek Models from V3 to V3.2 Understanding How DeepSeek's Flagship Open-Weight Models Evolved Similar to DeepSeek V3, the team released their new flagship model over a major US holiday weekend. Given DeepSeek V3.2's really good performance (on GPT-5 and Gemini 3.0 Pro) level, and the fact t... Nov 12, 2025 Recommendations for Getting the Most Out of a Technical Book This short article compiles a few notes I previously shared when readers ask how to get the most out of my building large language model from scratch books. I follow a similar approach when I read ... Nov 4, 2025 Beyond Standard LLMs Linear Attention Hybrids, Text Diffusion, Code World Models, and Small Recursive Transformers After I shared my Big LLM Architecture Comparison a few months ago, which focused on the main transformer-based LLMs, I received a lot of questions with respect to what I think about alternative ap... Oct 29, 2025 DGX Spark and Mac Mini for Local PyTorch Development First Impressions and Benchmarks The DGX Spark for local LLM inferencing and fine-tuning was a pretty popular discussion topic recently. I got to play with one myself, primarily working with and on LLMs in PyTorch, and collected s... Oct 5, 2025 Understanding the 4 Main Approaches to LLM Evaluation (From Scratch) Multiple-Choice Benchmarks, Verifiers, Leaderboards, and LLM Judges with Code Examples Multiple-Choice Benchmarks, Verifiers, Leaderboards, and LLM Judges with Code Examples Sep 6, 2025 Understanding and Implementing Qwen3 From Scratch A Detailed Look at One of the Leading Open-Source LLMs Previously, I compared the most notable open-weight architectures of 2025 in The Big LLM Architecture Comparison. Then, I zoomed in and discussed the various architecture components in From GPT-2 t... Aug 9, 2025 From GPT-2 to gpt-oss: Analyzing the Architectural Advances And How They Stack Up Against Qwen3 OpenAI just released their new open-weight LLMs this week: gpt-oss-120b and gpt-oss-20b, their first open-weight models since GPT-2 in 2019. And yes, thanks to some clever optimizations, they can r... Jul 19, 2025 The Big LLM Architecture Comparison From DeepSeek-V3 to Kimi K2: A Look At Modern LLM Architecture Design It has been seven years since the original GPT architecture was developed. At first glance, looking back at GPT-2 (2019) and forward to DeepSeek-V3 and Llama 4 (2024-2025), one might be surprised a... Jul 1, 2025 LLM Research Papers: The 2025 List (January to June) A topic-organized collection of 200+ LLM research papers from 2025 The latest in LLM research with a hand-curated, topic-organized list of over 200 research papers from 2025. Jun 17, 2025 Understanding and Coding the KV Cache in LLMs from Scratch KV caches are one of the most critical techniques for efficient inference in LLMs in production. KV caches are an important component for compute-efficient LLM inference in production.",
        "url": "https://sebastianraschka.com/blog",
        "page_type": "subpage"
      },
      {
        "title": "A Technical Tour of the DeepSeek Models from V3 to V3.2 | Sebastian Raschka, PhD",
        "description": "Similar to DeepSeek V3, the team released their new flagship model over a major US holiday weekend. Given DeepSeek V3.2’s really good performance (on GPT-5 and Gemini 3.0 Pro) level, and the fact that it’s also available as an open-weight model, it’s definitely",
        "sections": [
          {
            "heading": "1. The DeepSeek Release Timeline",
            "content": "While DeepSeek V3 wasn’t popular immediately upon release in December 2024, the DeepSeek R1 reasoning model (based on the identical architecture, using DeepSeek V3 as a base model) helped DeepSeek become one of the most popular open-weight models and a legit alternative to proprietary models such as the ones by OpenAI, Google, xAI, and Anthropic. Figure 2: DeepSeek V3 and R1 architecture from December 2024. We will revisit and discuss these architectural details in a later section. So, what’s new since V3/R1? I am sure that the DeepSeek team has been super busy this year. However, there hasn’t been a major release in the last 10-11 months since DeepSeek R1. Personally, I think it’s reasonable to go ~1 year for a major LLM release since it’s A LOT of work. However, I saw on various social media platforms that people were pronouncing the team “dead” (as a one-hit wonder). I am sure the DeepSeek team has also been busy navigating the switch from NVIDIA to Huawei chips. By the way, I am no"
          },
          {
            "heading": "2. Hybrid Versus Dedicated Reasoning Models",
            "content": "Before discussing further model details, it might be worthwhile to discuss the overall model types. Originally, DeepSeek V3 was released as a base model, and DeepSeek R1 added additional post-training to develop a dedicated reasoning model. This procedure is summarized in the figure below. Figure 4: Overview of the DeepSeek R1 training pipeline. This figure is from my more detailed Understanding Reasoning LLMs article. You can read more about the training pipeline in the figure above in my Understanding Reasoning LLMs article. What’s worthwhile noting here is that DeepSeek V3 is a base model, and DeepSeek R1 is a dedicated reasoning model. In parallel with DeepSeek, other teams have also released many really strong open-weight reasoning models. One of the strongest open-weight models this year was Qwen3. Originally, it was released as a hybrid reasoning model, which means that users were able to toggle between reasoning and non-reasoning modes within the same model. (In the case of Qwe"
          },
          {
            "heading": "3. From DeepSeek V3 to V3.1",
            "content": "Before discussing the new DeepSeek V3.2 release in more detail, I thought it would be helpful to start with an overview of the main changes going from V3 to V3.1. I already discussed DeepSeek V3 and R1 in great detail in several other articles. To summarize the main points, DeepSeek V3 is a base model that uses two noteworthy architecture aspects: Mixture-of-Experts (MoE) and Multi-Head Latent Attention (MLA). I think you are probably well familiar with MoE at this point, so I am skipping the introduction here. However, if you want to read more, I recommend the short overview in my The Big Architecture Comparison article for more context. The other noteworthy highlight is the use of MLA. MLA, which is used in DeepSeek V2, V3, and R1 , offers a memory-saving strategy that pairs particularly well with KV caching. The idea in MLA is that it compresses the key and value tensors into a lower-dimensional space before storing them in the KV cache. At inference time, these compressed tensors a"
          },
          {
            "heading": "3.2 DeepSeek R1 Overview and Reinforcement Learning with Verifiable Rewards (RLVR)",
            "content": "DeepSeek R1 uses the same architecture as DeepSeek V3 above. The difference is the training recipe. I.e., using DeepSeek V3 as the base model, DeepSeek R1 was focused on the Reinforcement Learning with Verifiable Rewards (RLVR) method to improve the reasoning capabilities of the model. The core idea in RLVR is to have the model learn from responses that can be verified symbolically or programmatically, such as math and code (but this can, of course, also be extended beyond these two domains). Figure 7: An example of a verifiable task. The GRPO algorithm, which is short for Group Relative Policy Optimization, is essentially a simpler variant of the Proximal Policy Optimization (PPO) algorithm that is popular in Reinforcement Learning with Human Feedback (RLHF), which is used for LLM alignment. Figure 8: Comparison of reinforcement learning setups in LLM training. Traditional RLHF with PPO uses both a reward model (trained on human preferences) and a critic (value model) to guide learnin"
          },
          {
            "heading": "3.3 DeepSeek R1-0528 Version Upgrade",
            "content": "As the DeepSeek team stated themselves, DeepSeek R1-0528 is basically a “minor version upgrade.” The architecture remains the same as in DeepSeek V3/R1, and the improvements are on the training side to bring it up to par with OpenAI o3 and Gemini 2.5 Pro at the time. Unfortunately, the DeepSeek team didn’t release any specific information describing how this was achieved; however, they stated that it partly comes from optimizations in their post-training pipeline. Also, based on what’s been shared, I think it’s likely that the hosted version of the model uses more computational resources at inference time (longer reasoning)."
          },
          {
            "heading": "3.4 DeepSeek V3.1 Hybrid Reasoning",
            "content": "DeepSeek V3.1 is a hybrid model with both general chat (instruct) and reasoning capabilities. I.e., instead of developing two separate models, there is now one model in which users can switch modes via the chat prompt template (similar to the initial Qwen3 model). DeepSeek V3.1 is based on DeepSeek V3.1-Base, which is in turn based on DeepSeek V3. They all share the same architecture."
          },
          {
            "heading": "4. DeepSeek V3.2-Exp and Sparse Attention",
            "content": "DeepSeek V3.2-Exp (Sep 2025) is where it gets more interesting. Originally, the DeepSeek V3.2-Exp didn’t top the benchmarks, which is why there wasn’t as much excitement around this model upon release. However, as I speculated back in September, this was likely an early, experimental release to get the infrastructure (especially the inference and deployment tools) ready for a larger release, since there are a few architectural changes in DeepSeek V3.2-Exp. The bigger release is DeepSeek V3.2 (not V4), but more on that later. So, what’s new in DeepSeek V3.2-Exp? First, DeepSeek V3.2-Exp was trained based on DeepSeek V3.1-Terminus as a base model. What’s DeepSeek V3.1-Terminus? It’s just a small improvement over the DeepSeek V3.1 checkpoint mentioned in the previous section. The technical report states that: DeepSeek-V3.2-Exp, an experimental sparse-attention model, which equips\nDeepSeek-V3.1-Terminus with DeepSeek Sparse Attention (DSA) through continued train-\ning. With DSA, a fine-gra"
          },
          {
            "heading": "5. DeepSeekMath V2 with Self-Verification and Self-Refinement",
            "content": "Having discussed DeepSeek V3.2-Exp, we are getting closer to the main topic of this article: DeepSeek V3.2. However, there is one more puzzle piece to discuss first. On November 27, 2025 (Thanksgiving in the US), and just 4 days before the DeepSeek V3.2 release, the DeepSeek team released DeepSeekMath V2 , based on DeepSeek V3.2-Exp-Base. This model was specifically developed for math and achieved gold-level scores in several math competitions. Essentially, we can think of it as a proof (of concept) model for DeepSeek V3.2, introducing one more technique. The key aspect here is that reasoning models (like DeepSeek R1 and others) are trained with an external verifier, and the model learns, by itself, to write explanations before arriving at the final answer. However, the explanations may be incorrect. As the DeepSeek team succinctly states, the shortcomings of regular RLVR: […] correct answers don’t guarantee correct reasoning. […] a model can arrive at the correct answer through flawed"
          },
          {
            "heading": "5.1 Self-Verification",
            "content": "Having an LLM score for the intermediate steps is not new. There is a whole line of research on so-called process reward models, which have focused on this. Examples include Solving Math Word Problems With Process- and Outcome-based Feedback (2022) or Let’s Verify Step by Step (2023) , but there are many more. The challenges with process reward models are that it’s not easy to check whether intermediate rewards are correct, and it can also lead to reward hacking. In the DeepSeek R1 paper in Jan 2025, they didn’t use process reward models as they found that: its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments. In this paper, they successfully revisit this in the form of self-verification. The motivation is that, even if no reference solution exists, humans can self-correct when reading proofs and identifying issues. So, in order to develop a better model for writing mathematic"
          },
          {
            "heading": "5.2 Self-Refinement",
            "content": "In the previous section, we talked about self-verification, i.e., analyzing the quality of the solution. The purpose of this is to implement self-refinement, which means that the LLM can act upon the feedback and revise its answer. Traditionally, in self-refinement, which is an established and popular inference-scaling technique, we would use the same LLM for generating the solution and verifying it, before refining it. In other words, in the previous figures 12 and 13, LLM 1 and LLM 2 would be the same LLM. So, a traditional self-refinement process would look as follows: Figure 14: A classic self-refinement iteration where the same LLM generates the initial response (Output 1), evaluates it (Eval), and produces a refined answer (Output 2). However, the DeepSeek team observed a crucial issue with using the same LLM for both the generation and verification in practice: when prompted to both generate and analyze its own proof in one shot, the generator tends to claim correctness even whe"
          }
        ],
        "content": "Similar to DeepSeek V3, the team released their new flagship model over a major US holiday weekend. Given DeepSeek V3.2’s really good performance (on GPT-5 and Gemini 3.0 Pro) level, and the fact that it’s also available as an open-weight model, it’s definitely Figure 1: Benchmark comparison between DeepSeek V3.2 and proprietary flagship models. This is an annotated figure from the DeepSeek V3.2 report I covered the predecessor, DeepSeek V3, at the very beginning of my The Big LLM Architecture Comparison article, which I kept extending over the months as new architectures got released. Originally, as I just got back from Thanksgiving holidays with my family, I planned to “just” extend the article with this new DeepSeek V3.2 release by adding another section, but I then realized that there’s just too much interesting information to cover, so I decided to make this a longer, standalone article. There’s a lot of interesting ground to cover and a lot to learn from their technical reports, so let’s get started! Table of Contents 1. The DeepSeek Release Timeline 2. Hybrid Versus Dedicated Reasoning Models 3. From DeepSeek V3 to V3.1 3.1 DeepSeek V3 Overview and Multi-Head Latent Attention (MLA) 3.2 DeepSeek R1 Overview and Reinforcement Learning with Verifiable Rewards (RLVR) 3.3 DeepSeek R1-0528 Version Upgrade 3.4 DeepSeek V3.1 Hybrid Reasoning 4. DeepSeek V3.2-Exp and Sparse Attention 5. DeepSeekMath V2 with Self-Verification and Self-Refinement 5.1 Self-Verification 5.2 Self-Refinement 6. DeepSeek V3.2 (Dec 1, 2025) 6.1 DeepSeek V3.2 Architecture 6.2 Reinforcement Learning Updates 6.3 GRPO Updates 6.4 DeepSeek V3.2-Speciale and Extended Thinking 7. Conclusion 1. The DeepSeek Release Timeline While DeepSeek V3 wasn’t popular immediately upon release in December 2024, the DeepSeek R1 reasoning model (based on the identical architecture, using DeepSeek V3 as a base model) helped DeepSeek become one of the most popular open-weight models and a legit alternative to proprietary models such as the ones by OpenAI, Google, xAI, and Anthropic. Figure 2: DeepSeek V3 and R1 architecture from December 2024. We will revisit and discuss these architectural details in a later section. So, what’s new since V3/R1? I am sure that the DeepSeek team has been super busy this year. However, there hasn’t been a major release in the last 10-11 months since DeepSeek R1. Personally, I think it’s reasonable to go ~1 year for a major LLM release since it’s A LOT of work. However, I saw on various social media platforms that people were pronouncing the team “dead” (as a one-hit wonder). I am sure the DeepSeek team has also been busy navigating the switch from NVIDIA to Huawei chips. By the way, I am not affiliated with them or have spoken with them; everything here is based on public information . As far as I know, they are back to using NVIDIA chips. Finally, it’s also not that they haven’t released anything. There have been a couple of smaller releases that trickled in this y",
        "url": "https://sebastianraschka.com/blog/2025/technical-deepseek.html",
        "page_type": "subpage"
      },
      {
        "title": "Recommendations for Getting the Most Out of a Technical Book | Sebastian Raschka, PhD",
        "description": "This short article compiles a few notes I previously shared when readers ask how to get the most out of my building large language model from scratch books. I follow a similar approach when I read technical books myself. It is not meant as a universal recipe, but it may be a helpful starting point. For this particular book, I strongly suggest reading it in order since each chapter depends on the previous one. And for each chapter, I recommend the following steps.",
        "sections": [
          {
            "heading": "3) Exercises",
            "content": "After the second read-through, retyping and running the code, it’s usually a good time to try the exercises. It’s great for solidifying one’s understanding or tinkering with a problem in a semi-structured way. If the exercise is too challenging, it’s okay to look at the solution. However, I would still recommend giving it a solid try first."
          },
          {
            "heading": "4) Review notes and explore further",
            "content": "Now, after reading the chapter, running the code, and doing the exercises, I recommend going back to highlights and annotations from the previous two read-throughs and seeing if there’s still something unclear. This is also a good time to look up additional references or do a quick search to clarify anything that still feels unresolved. But even if everything makes sense, reading more about a topic of interest is not a bad idea. At this stage, it also makes sense to write down or transfer useful insights, code snippets, etc., to your favorite note-taking app."
          },
          {
            "heading": "5) Use the ideas in a project",
            "content": "The previous steps were all about soaking up knowledge. Now, see if you can use certain aspects of a chapter in your own project. Or maybe build a small project using the code from the book as a starting point. For inspiration, check out the bonus materials, which are basically mini-projects I did to satisfy my own curiosity. For example, after reading about the multi-head attention mechanisms and implementing the LLM, you may wonder how well a model with grouped-query attention performs, or how much of a difference RMSNorm vs LayerNorm really makes. And so forth. There could also be smaller aspects that could be useful in your own projects. For example, sometimes it is a tiny detail that ends up being useful, like testing whether\nexplicitly calling torch.mps.manual_seed(seed) changes anything\ncompared to using torch.manual_seed(seed) alone."
          },
          {
            "heading": "Additional thoughts",
            "content": "Of course, none of the above is set in stone. If the topic is overall very familiar or easy, and I am primarily reading the book to get some information in later chapters, skimming a chapter is ok (to not waste my time). Also, for chapters that don’t have any code (for example, the introductory chapter 1), it makes of course sense to skip the code-related steps. Anyway, I hope this is useful. And happy reading and learning!"
          }
        ],
        "content": "Below are a few notes I previously shared when readers ask how to get the most out of my building large language model from scratch book(s). I follow a similar approach when I read technical books myself. It is not meant as a universal recipe, but it may be a helpful starting point. For this particular book, I strongly suggest reading it in order since each chapter depends on the previous one. And for each chapter, I recommend the following steps. I recommend reading the chapter from start to finish without any coding, yet. The goal of this first read-through is to get the big picture first. Ideally, I recommend reading the chapter away from the computer. A physical copy works well, but a digital device without distractions (no browser, social media, or email) works, too. Personally, I read both on paper and on an e-ink tablet. While I have used e-ink tablets since 2018, and always try to read more on e-ink, I still notice that physical copies help me focus better. That is also why I sometimes print research papers that are challenging or that I really want to understand in detail. My recommendation is to make the first read-through a short, focused 20-minute reading session with minimal distractions and without overthinking it or getting stuck with details. Highlighting or annotating confusing or interesting parts is fine, but I would not look things up at this stage. I just suggest reading, but not running any code yet. This first pass is meant to understand the bigger picture. On the second read-through, I recommend typing up and running the code from the chapter. Copying code is tempting because retyping is a lot of work, but when I read other technical books, it usually helps me to think about the code a bit more (versus just glancing over it). If I get different results than in the book, I would check the book’s GitHub repo and try the code from there. If I still get different results, I would try to see if it’s due to different package versions, random seeds, CPU/CUDA, etc. If I then still can’t figure it out, asking the author would not be a bad idea (via the book forum, public GitHub repo issues or discussions, and as a last resort, email). 3) Exercises After the second read-through, retyping and running the code, it’s usually a good time to try the exercises. It’s great for solidifying one’s understanding or tinkering with a problem in a semi-structured way. If the exercise is too challenging, it’s okay to look at the solution. However, I would still recommend giving it a solid try first. 4) Review notes and explore further Now, after reading the chapter, running the code, and doing the exercises, I recommend going back to highlights and annotations from the previous two read-throughs and seeing if there’s still something unclear. This is also a good time to look up additional references or do a quick search to clarify anything that still feels unresolved. But even if everything makes sense, reading more about a topic of interest is not",
        "url": "https://sebastianraschka.com/blog/2025/reading-books.html",
        "page_type": "subpage"
      },
      {
        "title": "Beyond Standard LLMs | Sebastian Raschka, PhD",
        "description": "After I shared my Big LLM Architecture Comparison a few months ago, which focused on the main transformer-based LLMs, I received a lot of questions with respect to what I think about alternative approaches. (I also recently gave a short talk about that at the PyTorch Conference 2025, where I also promised attendees to follow up with a write-up of these alternative approaches). So here it is!",
        "sections": [
          {
            "heading": "1. Transformer-Based LLMs",
            "content": "Transformer-based LLMs based on the classic Attention Is All You Need architecture are still state-of-the-art across text and code. If we just consider some of the highlights from late 2024 to today, notable models include DeepSeek V3/R1 OLMo 2 Gemma 3 Mistral Small 3.1 Llama 4 Qwen3 SmolLM3 Kimi K2 gpt-oss GLM-4.5 GLM-4.6 MiniMax-M2 (The list above focuses on the open-weight models; there are proprietary models like GPT-5, Grok 4, Gemini 2.5, etc. that also fall into this category.) Figure 2: An overview of the most notable decoder-style transformers released in the past year. Since I talked and wrote about transformer-based LLMs so many times, I assume you are familiar with the broad idea and architecture. If you’d like a deeper coverage, I compared the architectures listed above (and shown in the figure below) in my The Big LLM Architecture Comparison article. (Side note: I could have grouped Qwen3-Next and Kimi Linear with the other transformer-state space model (SSM) hybrids in th"
          },
          {
            "heading": "2. (Linear) Attention Hybrids",
            "content": "Before we discuss the “more different” approaches, let’s first look at transformer-based LLMs that have adopted more efficient attention mechanisms. In particular, the focus is on those that scale linearly rather than quadratically with the number of input tokens. There’s recently been a revival in linear attention mechanisms to improve the efficiency of LLMs. The attention mechanism introduced in the Attention Is All You Need paper (2017), aka scaled-dot-product attention, remains the most popular attention variant in today’s LLMs. Besides traditional multi-head attention, it’s also used in the more efficient flavors like grouped-query attention, sliding window attention, and multi-head latent attention as discussed in my talk ."
          },
          {
            "heading": "2.1 Traditional Attention and Quadratic Costs",
            "content": "The original attention mechanism scales quadratically with the sequence length: This is because the query (Q), key (K), and value (V) are n -by- d matrices, where d is the embedding dimension (a hyperparameter) and n is the sequence length (i.e., the number of tokens). (You can find more details in my Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs article ) Figure 4: Illustration of the traditional scaled-dot-product attention mechanism in multi-head attention; the quadratic cost in attention due to sequence length n."
          },
          {
            "heading": "2.2 Linear attention",
            "content": "Linear attention variants have been around for a long time, and I remember seeing tons of papers in the 2020s. For example, one of the earliest I recall is the 2020 Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention paper, where the researchers approximated the attention mechanism: Here, ϕ(⋅) is a kernel feature function, set to ϕ(x) = elu(x)+1. This approximation is efficient because it avoids explicitly computing the n×n attention matrix QKT. I don’t want to dwell too long on these older attempts. But the bottom line was that they reduced both time and memory complexity from \\(O(n^2)\\) to \\(O(n)\\) to make attention much more efficient for long sequences. However, they never really gained traction as they degraded the model accuracy, and I have never really seen one of these variants applied in an open-weight state-of-the-art LLM."
          },
          {
            "heading": "2.3 Linear Attention Revival",
            "content": "In the second half of this year, there has been revival of linear attention variants, as well as a bit of a back-and-forth from some model developers as illustrated in the figure below. Figure 5: An overview of the linear attention hybrid architectures. The first notable model was MiniMax-M1 with lightning attention. MiniMax-M1 is a 456B parameter mixture-of-experts (MoE) model with 46B active parameters, which came out back in June. Then, in August, the Qwen3 team followed up with Qwen3-Next, which I discussed in more detail above. Then, in September, the DeepSeek Team announced DeepSeek V3.2 . (DeepSeek V3.2 sparse attention mechanism is not strictly linear but at least subquadratic in terms of computational costs, so I think it’s fair to put it into the same category as MiniMax-M1, Qwen3-Next, and Kimi Linear.) All three models (MiniMax-M1, Qwen3-Next, DeepSeek V3.2) replace the traditional quadratic attention variants in most or all of their layers with efficient linear variants. I"
          },
          {
            "heading": "2.4 Qwen3-Next",
            "content": "Let’s start with Qwen3-Next, which replaced the regular attention mechanism by a Gated DeltaNet + Gated Attention hybrid, which helps enable the native 262k token context length in terms of memory usage (the previous 235B-A22B model model supported 32k natively, and 131k with YaRN scaling.) Their hybrid mechanism mixes Gated DeltaNet blocks with Gated Attention blocks within a 3:1 ratio as shown in the figure below. Figure 6: Qwen3-Next with gated attention and Gated DeltaNet. As depicted in the figure above, the attention mechanism is either implemented as gated attention or Gated DeltaNet. This simply means the 48 transformer blocks (layers) in this architecture alternate between this. Specifically, as mentioned earlier, they alternate in a 3:1 ratio. For instance, the transformer blocks are as follows: ──────────────────────────────────\nLayer 1 : Linear attention → MoE\nLayer 2 : Linear attention → MoE\nLayer 3 : Linear attention → MoE\nLayer 4 : Full attention → MoE\n──────────────────"
          },
          {
            "heading": "2.5 Gated Attention",
            "content": "Before we get to the Gated DeltaNet itself, let’s briefly talk about the gate. As you can see in the upper part of the Qwen3-Next architecture in the previous figure, Qwen3-Next uses “gated attention”. This is essentially regular full attention with an additional sigmoid gate. This gating is a simple modification that I added to an MultiHeadAttention implementation (based on code from chapter 3 of my LLMs from Scratch book ) below for illustration purposes: import torch from torch import nn class GatedMultiHeadAttention ( nn . Module ): def __init__ ( self , d_in , d_out , context_length , dropout , num_heads , qkv_bias = False ): super (). __init__ () assert d_out % num_heads == 0 self . d_out = d_out self . num_heads = num_heads self . head_dim = d_out // num_heads self . W_query = nn . Linear ( d_in , d_out , bias = qkv_bias ) #################################################### ### NEW: Add gate self . W_gate = nn . Linear ( d_in , d_out , bias = qkv_bias ) ########################"
          },
          {
            "heading": "2.6 Gated DeltaNet",
            "content": "Now, what is Gated DeltaNet? Gated DeltaNet (short for Gated Delta Network ) is Qwen3-Next’s linear-attention layer, which is intended as an alternative to standard softmax attention. It was adopted from the Gated Delta Networks: Improving Mamba2 with Delta Rule paper as mentioned earlier. Gated DeltaNet was originally proposed as an improved version of Mamba2, where it combines the gated decay mechanism of Mamba2 with a delta rule. Mamba is a state-space model (an alternative to transformers), a big topic that deserves separate coverage in the future. The delta rule part refers to computing the difference (delta, Δ) between new and predicted values to update a hidden state that is used as a memory state (more on that later). (Side note: Readers with classic machine learning literature can think of this as similar to Hebbian learning inspired by biology: “Cells that fire together wire together.” It’s basically a precursor of the perceptron update rule and gradient descent-based learnin"
          },
          {
            "heading": "2.7 DeltaNet Memory Savings",
            "content": "In the previous section, we discussed the advantage of the DeltaNet over full attention in terms of linear instead of quadratic compute complexity with respect to the context length. Next to the linear compute complexity, another big advantage of DeltaNet is the memory savings, as DeltaNet modules don’t grow the KV cache. (For more information about KV caching, see my Understanding and Coding the KV Cache in LLMs from Scratch article). Instead, as mentioned earlier, they keep a fixed-size recurrent state, so memory stays constant with context length. For a regular multi-head attention (MHA) layer, we can compute the KV cache size as follows: KV_cache_MHA ≈ batch_size × n_tokens × n_heads × d_head × 2 × bytes (The 2 multiplier is there because we have both keys and values that we store in the cache.) For the simplified DeltaNet version implemented above, we have: KV_cache_DeltaNet = batch_size × n_heads × d_head × d_head × bytes Note that the KV_cache_DeltaNet memory size doesn’t have a"
          },
          {
            "heading": "2.8 Kimi Linear vs. Qwen3-Next",
            "content": "Kimi Linear shares several structural similarities with Qwen3-Next. Both models rely on a hybrid attention strategy. Concretely, they combine lightweight linear attention with heavier full attention layers. Specifically, both use a 3:1 ratio, meaning for every three transformer blocks employing the linear Gated DeltaNet variant, there’s one block that uses full attention as shown in the figure below. Figure 11: Qwen3-Next and Kimi Linear side by side. Gated DeltaNet is a linear attention variant with inspiration from recurrent neural networks, including a gating mechanism from the Gated Delta Networks: Improving Mamba2 with Delta Rule paper. In a sense, Gated DeltaNet is a DeltaNet with Mamba-style gating, and DeltaNet is a linear attention mechanism (more on that in the next section) The MLA in Kimi Linear, depicted in the upper right box in the Figure 11 above, does not use the sigmoid gate.This omission was intentional so that the authors could compare the architecture more directly"
          }
        ],
        "content": "From DeepSeek R1 to MiniMax-M2, the largest and most capable open-weight LLMs today remain autoregressive decoder-style transformers, which are built on flavors of the original multi-head attention mechanism. However, we have also seen alternatives to standard LLMs popping up in recent years, from text diffusion models to the most recent linear attention hybrid architectures. Some of them are geared towards better efficiency, and others, like code world models, aim to improve modeling performance. After I shared my Big LLM Architecture Comparison a few months ago, which focused on the main transformer-based LLMs, I received a lot of questions with respect to what I think about alternative approaches. (I also recently gave a short talk about that at the PyTorch Conference 2025, where I also promised attendees to follow up with a write-up of these alternative approaches). So here it is! Figure 1: Overview of the LLM landscape. This article covers those architectures surrounded by the black frames. The decoder-style transformers are covered in my “The Big Architecture Comparison” article. Other non-framed architectures may be covered in future articles. Note that ideally each of these topics shown in the figure above would deserve at least a whole article itself (and hopefully get it in the future). So, to keep this article at a reasonable length, many sections are reasonably short. However, I hope this article is still useful as an introduction to all the interesting LLM alternatives that emerged in recent years. Table of Contents 1. Transformer-Based LLMs 2. (Linear) Attention Hybrids 2.1 Traditional Attention and Quadratic Costs 2.2 Linear attention 2.3 Linear Attention Revival 2.4 Qwen3-Next 2.5 Gated Attention 2.6 Gated DeltaNet 2.7 DeltaNet Memory Savings 2.8 Kimi Linear vs. Qwen3-Next 2.9 Kimi Delta Attention 2.10 The Future of Attention Hybrids 3. Text Diffusion Models 3.1 Why Work on Text Diffusion? 3.2 The Denoising Process 3.3 Autoregressive vs Diffusion LLMs 3.4 Text Diffusion Today 4. World Models 4.1 The Main Idea Behind World Models 4.2 From Vision to Code 4.3 Code World Models Vs Regular LLMs for Code 5. Small Recursive Transformers 5.1 What Does Recursion Mean Here? 5.2 How Does TRM Differ From HRM? 5.3 The Bigger Picture 6. Conclusion PS: The aforementioned PyTorch conference talk will be uploaded to the official PyTorch YouTube channel. In the meantime, if you are curious, you can find a practice recording version below. 1. Transformer-Based LLMs Transformer-based LLMs based on the classic Attention Is All You Need architecture are still state-of-the-art across text and code. If we just consider some of the highlights from late 2024 to today, notable models include DeepSeek V3/R1 OLMo 2 Gemma 3 Mistral Small 3.1 Llama 4 Qwen3 SmolLM3 Kimi K2 gpt-oss GLM-4.5 GLM-4.6 MiniMax-M2 and many more. (The list above focuses on the open-weight models; there are proprietary models like GPT-5, Grok 4, Gemini 2.5, etc. that also fall into this ",
        "url": "https://sebastianraschka.com/blog/2025/beyond-standard-llms.html",
        "page_type": "subpage"
      },
      {
        "title": "DGX Spark and Mac Mini for Local PyTorch Development | Sebastian Raschka, PhD",
        "description": "The DGX Spark for local LLM inferencing and fine-tuning was a pretty popular discussion topic recently. I got to play with one myself, primarily working with and on LLMs in PyTorch, and collected some benchmarks and takeaways.",
        "sections": [
          {
            "heading": "The Usual Use Case: Local Inference",
            "content": "Most people use the DGX Spark for local inference with tools like Ollama . That’s also what I did previously on my Mac Mini. The DGX feels similar here but with one major difference: it has 128 GB of VRAM, which makes it possible to run larger models beyond the gpt-oss-20B model that I typically use. For an apples-to-apples comparison though, in Ollama with optimized mxfp4 precision (for MoE models), the DGX Spark and Mac Mini M4 Pro achieve roughly 45 tok/sec when running gpt-oss-20B . My benchmarks below are more focused on PyTorch, but if you are curious about the Ollama use case, this blog post by LMSYS has more details. That said, what’s more interesting to me is using it as a prototyping and development machine for my pure PyTorch projects. Below are several benchmark comparisons with my Mac Mini, as well as H100 and A100 cards I typically use via cloud providers."
          },
          {
            "heading": "1. Inference with a 0.6B Model Implemented from Scratch",
            "content": "In this section, I am comparing the different machines on a small 0.6B LLM model I implemented from scratch in pure PyTorch. This is a model I currently use in my Build A Reasoning Model (From Scratch) book. In particular, I ran this 0.6 B parameter model for generating answers for simple prompts both with and without a KV-cache, and the results are shown below. Figure 2: A simple inference task where the model is prompted to generate a short 30-token response. Note: All experiments were run in PyTorch 2.9. The InductorError s I encountered when running compiled model on the Mac GPU (“mps” backend in PyTorch) are now resolved in 2.9. The DGX Spark vastly outperforms the Mac Mini M4 Pro and is roughly on par with the 6-times more expensive H100 data center GPU, which is impressive. Unfortunately, I couldn’t run the compiled versions on the Mac due to PyTorch MPS limitations. MPS is improving but still not on par with CUDA. Side note: By the way, this is a relatively small model, and the"
          },
          {
            "heading": "2. Evaluating a 0.6B Base vs Reasoning Model on MATH-500",
            "content": "This benchmark extends the previous one and compares a base model and a reasoning model across 500 MATH-500 prompts that produce answers of vastly different lengths. (I am using the uncompiled KV-cache version here.) The following plots show results for running the evaluation sequentially (one prompt at a time) or in batches (with 128 prompts at a time). Figure 3: Comparison of a base and reasoning model on MATH-500. The y-axis represents the total runtime, so lower is better. In general, the reasoning model is much slower than the base model as it generates much longer responses. The average response length of the base model is 96.74 tokens whereas the average response length of the reasoning model is 1361.21 tokens. As we can see in Figure 3, in the sequential runs (2a), the DGX Spark even outperformed the 6× more expensive H100, which is again impressive. However, when it comes to batched runs, the H100 is the clear winner. This is presumably because of its much better memory bandwi"
          },
          {
            "heading": "3. Training / Fine-Tuning a 355M Model",
            "content": "Previously, we have seen that the DGX Spark is great for single-sequence generation, but less ideal for large-batch inference compared to an H100. How about small training and post-training runs? I ran short pre-training (3a), supervised finetuning (3b), and DPO preference-tuning runs to compare the different system, as shown in the figure below. Figure 4: Comparisons across pre-training, supervised fine-tuning, and DPO preference tuning. Note that I ran these experiments on an A100 not on an H100 as I didn’t have an H100 available at the time. Across all three categories, the DGX Spark and A100 were both significantly faster than the Mac Mini. These are very short runs, but they show that the DGX could handle smaller-scale training and fine-tuning tasks efficiently. Links to the code to reproduce these runs can be found below: Pre-training (3a): https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/01_main-chapter-code (but change from 127M to 355M model.) SFT fine-tuning (3b): ht"
          },
          {
            "heading": "Conclusion",
            "content": "Overall, the DGX Spark seems to be a neat little workstation that can sit quietly next to a Mac Mini. It has a similarly small form factor, but with more GPU memory and of course (and importantly!) CUDA support. I previously had a Lambda workstation with 4 GTX 1080Ti GPUs in 2018. I needed the machine for my research, but the noise and heat in my office was intolerable, which is why I had to eventually move the machine to a dedicated server room at UW-Madison. After that, I didn’t consider buying another GPU workstation but solely relied on cloud GPUs. (I would perhaps only consider it again if I moved into a house with a big basement and a walled-off spare room.) The DGX Spark, in contrast, is definitely quiet enough for office use. Even under full load it’s barely audible. It also ships with software that makes remote use seamless and you can connect directly from a Mac without extra peripherals or SSH tunneling. That’s a huge plus for quick experiments throughout the day. But, of co"
          }
        ],
        "content": "The DGX Spark for local LLM inferencing and fine-tuning was a pretty popular discussion topic recently. I got to play with one myself, primarily working with and on LLMs in PyTorch, and collected some benchmarks and takeaways. Figure 1: The DGX next to my mini, with a tea pot (and a 13-inch MacBook Air) for scale. Both have roughly the same size and are super quiet (which is great for office or desk use). The Usual Use Case: Local Inference Most people use the DGX Spark for local inference with tools like Ollama . That’s also what I did previously on my Mac Mini. The DGX feels similar here but with one major difference: it has 128 GB of VRAM, which makes it possible to run larger models beyond the gpt-oss-20B model that I typically use. For an apples-to-apples comparison though, in Ollama with optimized mxfp4 precision (for MoE models), the DGX Spark and Mac Mini M4 Pro achieve roughly 45 tok/sec when running gpt-oss-20B . My benchmarks below are more focused on PyTorch, but if you are curious about the Ollama use case, this blog post by LMSYS has more details. That said, what’s more interesting to me is using it as a prototyping and development machine for my pure PyTorch projects. Below are several benchmark comparisons with my Mac Mini, as well as H100 and A100 cards I typically use via cloud providers. 1. Inference with a 0.6B Model Implemented from Scratch In this section, I am comparing the different machines on a small 0.6B LLM model I implemented from scratch in pure PyTorch. This is a model I currently use in my Build A Reasoning Model (From Scratch) book. In particular, I ran this 0.6 B parameter model for generating answers for simple prompts both with and without a KV-cache, and the results are shown below. Figure 2: A simple inference task where the model is prompted to generate a short 30-token response. Note: All experiments were run in PyTorch 2.9. The InductorError s I encountered when running compiled model on the Mac GPU (“mps” backend in PyTorch) are now resolved in 2.9. The DGX Spark vastly outperforms the Mac Mini M4 Pro and is roughly on par with the 6-times more expensive H100 data center GPU, which is impressive. Unfortunately, I couldn’t run the compiled versions on the Mac due to PyTorch MPS limitations. MPS is improving but still not on par with CUDA. Side note: By the way, this is a relatively small model, and the KV cache is dynamic and allocated at runtime to further reduce memory. This means the KV cache grows with the response length, instead of using a pre-allocated array, which is more optimal for the GPU and compilation. I implemented it in this dynamic way on purpose to reduce memory requirements, which is often the main bottleneck for most readers. This is why you may see oddities in the plot, like that the KV-cache version is slightly slower on the GPU than the non-KV-cache version. Here, the prompt is short enough that brute-forcing everything on the GPU is not a big deal (but you can see that the Mac Mini ",
        "url": "https://sebastianraschka.com/blog/2025/dgx-impressions.html",
        "page_type": "subpage"
      },
      {
        "title": "Understanding the 4 Main Approaches to LLM Evaluation (From Scratch) | Sebastian Raschka, PhD",
        "description": "Multiple-Choice Benchmarks, Verifiers, Leaderboards, and LLM Judges with Code Examples",
        "sections": [
          {
            "heading": "Understanding the main evaluation methods for LLMs",
            "content": "There are four common ways of evaluating trained LLMs in practice: multiple choice , verifiers , leaderboards , and LLM judges , as shown in Figure 1 below. Research papers, marketing materials, technical reports, and model cards (a term for LLM-specific technical reports) often include results from two or more of these categories. Figure 1: An overview of the 4 different evaluations models covered in this article. Furthermore the four categories introduced here fall into two groups: benchmark-based evaluation and judgment-based evaluation , as shown in the figure above. (There are also other measures, such as training loss, perplexity , and rewards , but they are usually used internally during model development.) The following subsections provide brief overviews and examples of each of the four methods."
          },
          {
            "heading": "Method 1: Evaluating answer-choice accuracy",
            "content": "We begin with a benchmark‑based method: multiple‑choice question answering. Historically, one of the most widely used evaluation methods is multiple-choice benchmarks such as MMLU (short for Massive Multitask Language Understanding, https://huggingface.co/datasets/cais/mmlu ). To illustrate this approach, figure 2 shows a representative task from the MMLU dataset. Figure 2: Evaluating an LLM on MMLU by comparing its multiple-choice prediction with the correct answer from the dataset. Figure 2 shows just a single example from the MMLU dataset. The complete MMLU dataset consists of 57 subjects (from high school math to biology) with about 16 thousand multiple-choice questions in total, and performance is measured in terms of accuracy (the fraction of correctly answered questions), for example 87.5% if 14,000 out of 16,000 questions are answered correctly. Multiple-choice benchmarks, such as MMLU, test an LLM’s knowledge recall in a straightforward, quantifiable way similar to standardize"
          },
          {
            "heading": "1.2 Loading the model",
            "content": "First, before we can evaluate it on MMLU, we have to load the pre-trained model. Here, we are going to use a from-scratch implementation of Qwen3 0.6B in pure PyTorch, which requires only about 1.5 GB of RAM. Note that the Qwen3 model implementation details are not important here; we simply treat it as an LLM we want to evaluate. However, if you are curious, a from-scratch implementation walkthrough can be found in my previous Understanding and Implementing Qwen3 From Scratch article, and the source code is also available here on GitHub . Instead of copy & pasting the many lines of Qwen3 source code, we import it from my reasoning_from_scratch Python library, which can be installed via pip install reasoning_from_scratch uv add reasoning_from_scratch Code block 1: Loading a pre-trained model from pathlib import Path import torch from reasoning_from_scratch.ch02 import get_device from reasoning_from_scratch.qwen3 import ( download_qwen3_small , Qwen3Tokenizer , Qwen3Model , QWEN_CONFIG_0"
          },
          {
            "heading": "1.3 Checking the generated answer letter",
            "content": "In this section, we implement the simplest and perhaps most intuitive MMLU scoring method, which relies on checking whether a generated multiple-choice answer letter matches the correct answer. This is similar to what was illustrated earlier in Figure 2, which is shown below again for convenience. Figure 3: Evaluating an LLM on MMLU by comparing its multiple-choice prediction with the correct answer from the dataset. For this, we will work with an example from the MMLU dataset: example = { \"question\" : ( \"How many ways are there to put 4 distinguishable\" \" balls into 2 indistinguishable boxes?\" ), “ choices ” : [ \"7\" , \"11\" , \"16\" , \"8\" ], “ answer ” : \"D\" , } Next, we define a function to format the LLM prompts. Code block 2: Loading a pre-trained model def format_prompt ( example ): return ( f \" { example [ 'question' ] } \\n \" f \"A. { example [ 'choices' ][ 0 ] } \\n \" f \"B. { example [ 'choices' ][ 1 ] } \\n \" f \"C. { example [ 'choices' ][ 2 ] } \\n \" f \"D. { example [ 'choices' ][ 3 "
          },
          {
            "heading": "Method 2: Using verifiers to check answers",
            "content": "Related to multiple-choice question answering discussed in the previous section, verification-based approaches quantify the LLMs capabilities via an accuracy metric. However, in contrast to multiple-choice benchmarks, verification methods allow LLMs to provide a free-form answer. We then extract the relevant answer portion and use a so-called verifier to compare the answer portion to the correct answer provided in the dataset, as illustrated in Figure 6 below. Figure 6: Evaluating an LLM with a verification-based method in free-form question answering. The model generates a free-form answer (which may include multiple steps) and a final boxed answer, which is extracted and compared against the correct answer from the dataset. When we compare the extracted answer with the provided answer, as shown in figure above, we can employ external tools, such as code interpreters or calculator-like tools/software. The downside is that this method can only be applied to domains that can be easily ("
          },
          {
            "heading": "Method 3: Comparing models using preferences and leaderboards",
            "content": "So far, we have covered two methods that offer easily quantifiable metrics such as model accuracy. However, none of the aforementioned methods evaluate LLMs in a more holistic way, including judging the style of the responses. In this section, as illustrated in Figure 8 below, we discuss a judgment-based method, namely, LLM leaderboards. Figure 8: A mental model of the topics covered in this book with a focus on the judgment- and benchmark-based evaluation methods covered in this appendix. Having already covered benchmark-based approaches (multiple choice, verifiers) in the previous section, we now introduce judgment-based approaches to measure LLM performance, with this subsection focusing on leaderboards. The leaderboard method described here is a judgment-based approach where models are ranked not by accuracy values or other fixed benchmark scores but by user (or other LLM) preferences on their outputs. A popular leaderboard is LM Arena (formerly Chatbot Arena ), where users compare"
          },
          {
            "heading": "Method 4: Judging responses with other LLMs",
            "content": "In the early days, LLMs were evaluated using statistical and heuristics-based methods, including a measure called BLEU , which is a crude measure of how well generated text matches reference text. The problem with such metrics is that they require exact word matches and don’t account for synonyms, word changes, and so on. One solution to this problem, if we want to judge the written answer text as a whole, is to use relative rankings and leaderboard-based approaches as discussed in the previous section. However, a downside of leaderboards is the subjective nature of the preference-based comparisons as it involves human feedback (as well as the challenges that are associated with collecting this feedback). A related method is to use another LLM with a pre-defined grading rubric (i.e., an evaluation guide) to compare an LLM’s response to a reference response and judge the response quality based on a pre-defined rubric, as illustrated in Figure 12. Figure F12: Example of an LLM-judge eval"
          },
          {
            "heading": "4.1 Implementing a LLM-as-a-judge approach in Ollama",
            "content": "Ollama is an efficient open-source application for running LLMs on a laptop. It serves as a wrapper around the open-source llama.cpp library, which implements LLMs in pure C/C++ to maximize efficiency. However, note that Ollama is only a tool for generating text using LLMs (inference) and does not support training or fine-tuning LLMs. To execute the following code, please install Ollama by visiting the official website at https://ollama.com and follow the provided instructions for your operating system: For macOS and Windows users: Open the downloaded Ollama application. If prompted to install command-line usage, select “yes.” For Linux users: Use the installation command available on the Ollama website. Before implementing the model evaluation code, let’s first download the gpt-oss model and verify that Ollama is functioning correctly by using it from the command line terminal. Execute the following command on the command line (not in a Python session) to try out the 20 billion parame"
          },
          {
            "heading": "Conclusion",
            "content": "In this article, we covered four different evaluation approaches: multiple choice, verifiers, leaderboards, and LLM judges. I know this was a long article, but I hope you found it useful for getting an overview of how LLMs are evaluated. A from-scratch approach like this can be verbose, but it is a great way to understand how these methods work under the hood, which in turn helps us identify weaknesses and areas for improvement. That being said, you are probably wondering, “What is the best way to evaluate an LLM?” Unfortunately, there is no single best method since, as we have seen, each comes with different trade-offs. In short: Multiple-choice (+) Relatively quick and cheap to run at scale\n(+) Standardized and reproducible across papers (or model cards)\n(-) Measures basic knowledge recall\n(-) Does not reflect how LLMs are used in the real world Verifiers (+) Standardized, objective grading for domains with ground truth\n(+) Allows free-form answers (with some constraints on final ans"
          }
        ],
        "content": "How do we actually evaluate LLMs? It’s a simple question, but one that tends to open up a much bigger discussion. When advising or collaborating on projects, one of the things I get asked most often is how to choose between different models and how to make sense of the evaluation results out there. (And, of course, how to measure progress when fine-tuning or developing our own.) Since this comes up so often, I thought it might be helpful to share a short overview of the main evaluation methods people use to compare LLMs. Of course, LLM evaluation is a very big topic that can’t be exhaustively covered in a single resource, but I think that having a clear mental map of these main approaches makes it much easier to interpret benchmarks, leaderboards, and papers. I originally planned to include these evaluation techniques in my upcoming book, Build a Reasoning Model (From Scratch) , but they ended up being a bit outside the main scope. (The book itself focuses more on verifier-based evaluation.) So I figured that sharing this as a longer article with from-scratch code examples would be nice. In Build A Reasoning Model (From Scratch) , I am taking a hands-on approach to building a reasoning LLM from scratch. If you liked “Build A Large Language Model (From Scratch)”, this book is written in a similar style in terms of building everything from scratch in pure PyTorch. Reasoning is one of the most exciting and important recent advances in improving LLMs, but it’s also one of the easiest to misunderstand if you only hear the term reasoning and read about it in theory. So, in this book , I am taking a hands-on approach to building a reasoning LLM from scratch. The book is currently in early-access with >100 pages already online, and I have just finished another 30 pages that are currently being added by the layout team. If you joined the early access program (a big thank you for your support!), you should receive an email when those go live. PS: There’s a lot happening on the LLM research front right now. I’m still catching up on my growing list of bookmarked papers and plan to highlight some of the most interesting ones in the next article. But now, let’s discuss the four main LLM evaluation methods along with their from-scratch code implementations to better understand their advantages and weaknesses. Table of contents Understanding the main evaluation methods for LLMs Method 1: Evaluating answer-choice accuracy 1.2 Loading the model Code block 1: Loading a pre-trained model 1.3 Checking the generated answer letter Code block 2: Loading a pre-trained model Code block 3: Extracting the generated letter Method 2: Using verifiers to check answers Method 3: Comparing models using preferences and leaderboards Code block 4: Constructing a leaderboard Method 4: Judging responses with other LLMs 4.1 Implementing a LLM-as-a-judge approach in Ollama Code block 5: Checking if Ollama is running Code block 6: Querying a local Ollama model Code block 7: Setting up th",
        "url": "https://sebastianraschka.com/blog/2025/llm-evaluation-4-approaches.html",
        "page_type": "subpage"
      },
      {
        "title": "Books | Sebastian Raschka, PhD",
        "description": "Overview of Sebastian Raschka’s books on machine learning, deep learning, and large language models, with links to purchase and supporting materials.",
        "sections": [
          {
            "heading": "Build a Reasoning Model (From Scratch) – In Progress",
            "content": "ISBN-13 9781633434677 Amazon (TBD) Manning (first chapters already available) In Build a Reasoning Model (from Scratch) , you will learn and understand how a reasoning large language model (LLM) works. Reasoning is one of the most exciting and important recent advances in improving LLMs, but it’s also one of the easiest to misunderstand if you only hear the term reasoning and read about it in theory. This is why this book takes a hands-on approach. We will start with a pre-trained base LLM and then add reasoning capabilities ourselves, step by step in code, so you can see exactly how it works. This book stands on its own, but it can also be read as a natural sequel to the best-selling Build a Large Language Model (from Scratch) . Whereas the earlier book focused on building and training a base model, this book begins with a pre-trained LLM and focuses on extending it with reasoning capabilities via inference-time scaling, reinforcement learning, and distillation. Link to the official s"
          },
          {
            "heading": "Build a Large Language Model (From Scratch)",
            "content": "ISBN-13 978-1633437166 In Build a Large Language Model (from Scratch) , you’ll discover how LLMs work from the inside out. In this book, I’ll guide you step by step through creating your own LLM, explaining each stage with clear text, diagrams, and examples. The method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT. The book uses Python and PyTorch for all its coding examples. Link to the official source code repository Yes, we can absolutely build applications while knowing very little about what an LLM actually is (just by calling APIs). But honestly, if you want to become a top-tier ML / AI Engineer, you need to understand what’s going on under the hood. And what better book to start with than one that explains how to build an actual LLM from scratch? –Via Miguel Otero Pedrido , Senior Machine Learning Engineer at Zapier "
          },
          {
            "heading": "Build a Large Language Model (From Scratch)Video Course",
            "content": "A 17-hour and 15-minute companion video course where I code through each chapter of the book. The course is organized into chapters and sections that mirror the book’s structure so that it can be used as a standalone alternative to the book or complementary code-along resource."
          },
          {
            "heading": "Machine Learning Q and AI",
            "content": "ISBN-10: 1718503768\nISBN-13: 978-1718503762\nPaperback: 264 pages\nNo Starch Press (March, 2024) No Starch Press book page Amazon book page If you’re ready to venture beyond introductory concepts and dig deeper into machine learning, deep learning, and AI, the question-and-answer format of Machine Learning Q and AI will make things fast and easy for you, without a lot of mucking about. Each brief, self-contained chapter journeys through a fundamental question in AI, unraveling it with clear explanations, diagrams, and exercises. Multi-GPU training paradigms Finetuning transformers Differences between encoder- and decoder-style LLMs Concepts behind vision transformers Confidence intervals for ML And many more! Supplementary Materials and Discussions This book is a fully edited and revised version of Machine Learning Q and AI , which was available on Leanpub. “Sebastian has a gift for distilling complex, AI-related topics into practical takeaways that can be understood by anyone. His new b"
          },
          {
            "heading": "Machine Learning with PyTorch and Scikit-Learn",
            "content": "ISBN-10: 1801819319\nISBN-13: 978-1801819312\nPaperback: 770 pages\nPackt Publishing Ltd. (February 25, 2022) Amazon.com book page Packt’s book page (the publisher) Initially, this project started as the 4th edition of Python Machine Learning. However, after putting so much passion and hard work into the changes and new topics, we thought it deserved a new title.\nSo, what’s new? There are many contents and additions, including the switch from TensorFlow to PyTorch, new chapters on graph neural networks and transformers, a new section on gradient boosting, and many more that I will detail in a separate blog post.\nFor those who are interested in knowing what this book covers in general, I’d describe it as a comprehensive resource on the fundamental concepts of machine learning and deep learning. The first half of the book introduces readers to machine learning using scikit-learn, the defacto approach for working with tabular datasets. Then, the second half of this book focuses on deep learn"
          },
          {
            "heading": "Older Books",
            "content": "You can find a list of all my books here ."
          }
        ],
        "content": "Build a Reasoning Model (From Scratch) – In Progress ISBN-13 9781633434677 Amazon (TBD) Manning (first chapters already available) Description In Build a Reasoning Model (from Scratch) , you will learn and understand how a reasoning large language model (LLM) works. Reasoning is one of the most exciting and important recent advances in improving LLMs, but it’s also one of the easiest to misunderstand if you only hear the term reasoning and read about it in theory. This is why this book takes a hands-on approach. We will start with a pre-trained base LLM and then add reasoning capabilities ourselves, step by step in code, so you can see exactly how it works. This book stands on its own, but it can also be read as a natural sequel to the best-selling Build a Large Language Model (from Scratch) . Whereas the earlier book focused on building and training a base model, this book begins with a pre-trained LLM and focuses on extending it with reasoning capabilities via inference-time scaling, reinforcement learning, and distillation. Other links Link to the official source code repository Build a Large Language Model (From Scratch) ISBN-13 978-1633437166 Amazon Manning Description In Build a Large Language Model (from Scratch) , you’ll discover how LLMs work from the inside out. In this book, I’ll guide you step by step through creating your own LLM, explaining each stage with clear text, diagrams, and examples. The method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT. The book uses Python and PyTorch for all its coding examples. Other links Link to the official source code repository Reviews Yes, we can absolutely build applications while knowing very little about what an LLM actually is (just by calling APIs). But honestly, if you want to become a top-tier ML / AI Engineer, you need to understand what’s going on under the hood. And what better book to start with than one that explains how to build an actual LLM from scratch? –Via Miguel Otero Pedrido , Senior Machine Learning Engineer at Zapier I got a serious closeup look at what goes on inside an LLM. every step of the way, the book surprised with great detail, reiteration, recap and very manageable chunks to internalize the ideas. –Via Ganapathy Subramaniam , Gen AI developer I have read many technical books in my career spanning 20+ years, but this is the best technical book I have ever studied by a large margin. So if you are someone who is looking for a in-depth explanation of internal workings and from the scratch development of Large Language Models, then this is the book you should be reading. –Via Soumitri Kadambi , Director Artificial Intelligence at ZeOmega ‘Build a Large Language Model from Scratch’ by Sebastian Raschka @rasbt has been an invaluable resource for me, connecting many dots and sparking numerous ‘aha’ moments. Thi",
        "url": "https://sebastianraschka.com/books",
        "page_type": "subpage"
      }
    ]
  },
  "secondary_content": {
    "source": "web_search",
    "reliability": "medium",
    "searches": []
  }
}